{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bafa60-29bb-428f-986f-d54a3887a4dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 11:51:11.039507: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-23 11:51:11.092162: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Ablation Study for TCR-peptide-MHC Binding Prediction\n",
      "Device: cuda\n",
      "Configuration: 3 runs, 10 epochs per run\n",
      "============================================================\n",
      "1. ESM Freezing Strategy Ablation\n",
      "============================================================\n",
      "Testing freeze_layers = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Ablation Study for TCR-peptide-MHC Binding Prediction\n",
    "# Based on Main Model B1: ESM Fine-tuning + InfoNCE + Temperature Scaling\n",
    "\n",
    "import os, sys, math, json, time, random, warnings\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import EsmModel, EsmTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please install tqdm: pip install tqdm\")\n",
    "\n",
    "# --------------------------\n",
    "# Configuration\n",
    "# --------------------------\n",
    "\n",
    "@dataclass\n",
    "class AblationConfig:\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    n_runs: int = 3\n",
    "    test_size: float = 0.2\n",
    "    val_size: float = 0.1\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 8\n",
    "    base_lr: float = 2e-4\n",
    "    \n",
    "    # ESM config\n",
    "    esm_model_name: str = \"facebook/esm2_t12_35M_UR50D\"\n",
    "    \n",
    "    # Ablation ranges\n",
    "    freeze_layers_options: List[int] = field(default_factory=lambda: [0, 3, 6, 9, 12])\n",
    "    infonce_weight_options: List[float] = field(default_factory=lambda: [0.0, 0.05, 0.1, 0.2])\n",
    "    d_model_options: List[int] = field(default_factory=lambda: [64, 128, 256])\n",
    "    calibration_options: List[str] = field(default_factory=lambda: [\"none\", \"temperature\", \"platt\"])\n",
    "    neg_ratio_options: List[int] = field(default_factory=lambda: [2, 4, 6, 8])\n",
    "    mhc_encoding_options: List[str] = field(default_factory=lambda: [\"pseudo_seq\", \"simple_embed\"])\n",
    "\n",
    "AA_STANDARD = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "AA_SET = set(AA_STANDARD)\n",
    "\n",
    "# MHC pseudo-sequences\n",
    "MHC_PSEUDO_SEQUENCES = {\n",
    "    \"HLA-A*02:01\": \"GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETLQRTDAPKTHMTHHAVSDHEATLRCWALSFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGQEQRYTCHVQHEGLPKPLTLRWE\",\n",
    "    \"HLA-A*01:01\": \"GSHSMRYFFTSVSRPGRGEPRFIAMGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDRETQKAKGNEQSFRVDLRTLLGYYNQSEDGSHTIQIMYGCDVGPDGRLLRGYDQYAYDGKDYIALNEDLRSWTAADTAAQITQRKWEAARVAEQLRAYLEGTCVEWLRRYLENGKDKLERADPPKTHVTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWE\",\n",
    "    \"HLA-B*07:02\": \"GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDRETQKAKGNEQSFRVDLRTLLGYYNQSEDGSHTIQIMYGCDVGPDGRLLRGYDQYAYDGKDYIALNEDLRSWTAADTAAQITQRKWEAARVAEQLRAYLEGTCVEWLRRYLENGKDKLERADPPKTHVTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWE\"\n",
    "}\n",
    "\n",
    "def get_mhc_sequence(mhc_name: str, encoding_type: str = \"pseudo_seq\") -> str:\n",
    "    if encoding_type == \"pseudo_seq\":\n",
    "        return MHC_PSEUDO_SEQUENCES.get(mhc_name, MHC_PSEUDO_SEQUENCES[\"HLA-A*02:01\"])\n",
    "    else:  # simple_embed\n",
    "        # Simple embedding: just use a short representative sequence\n",
    "        mhc_to_simple = {\n",
    "            \"HLA-A*02:01\": \"GSHSMRYFFTSV\",\n",
    "            \"HLA-A*01:01\": \"GSHSMRYFFAMT\", \n",
    "            \"HLA-B*07:02\": \"GSHSMRYFFBPT\"\n",
    "        }\n",
    "        return mhc_to_simple.get(mhc_name, \"GSHSMRYFFDEF\")\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# --------------------------\n",
    "# Data Management\n",
    "# --------------------------\n",
    "\n",
    "class AblationDataManager:\n",
    "    def __init__(self, config: AblationConfig):\n",
    "        self.config = config\n",
    "        self.df_train = None\n",
    "        self.df_val = None\n",
    "        self.df_test = None\n",
    "        \n",
    "    def load_and_split_data(self, data_path: str = \"data.csv\", neg_ratio: int = 4):\n",
    "        \"\"\"Load data and create train/val/test splits with specified negative ratio\"\"\"\n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"Data file {data_path} not found. Please ensure data is available.\")\n",
    "        \n",
    "        df = pd.read_csv(data_path)\n",
    "        df = self._preprocess_data(df)\n",
    "        df_with_negs = self._build_negatives(df, k_neg=neg_ratio)\n",
    "        self._split_data(df_with_negs)\n",
    "        \n",
    "    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and preprocess data\"\"\"\n",
    "        df = df.copy()\n",
    "        df[\"CDR3\"] = df[\"CDR3\"].astype(str).str.upper()\n",
    "        df[\"MHC\"] = df[\"MHC\"].astype(str).str.upper()\n",
    "        df[\"Epitope\"] = df[\"Epitope\"].astype(str).str.upper()\n",
    "        df = df.dropna().drop_duplicates()\n",
    "        df = df[df[\"Epitope\"].map(lambda p: all(ch in AA_SET for ch in p))]\n",
    "        df = df[df[\"CDR3\"].map(lambda p: all(ch in AA_SET for ch in p))]\n",
    "        return df\n",
    "    \n",
    "    def _build_negatives(self, df: pd.DataFrame, k_neg: int = 4) -> pd.DataFrame:\n",
    "        \"\"\"Generate negative samples\"\"\"\n",
    "        df = df.copy()\n",
    "        df[\"label\"] = 1\n",
    "        \n",
    "        # Group peptides by length\n",
    "        peps_by_len = defaultdict(list)\n",
    "        for pep in df[\"Epitope\"].unique():\n",
    "            peps_by_len[len(pep)].append(pep)\n",
    "        \n",
    "        negatives = []\n",
    "        for _, row in df.iterrows():\n",
    "            cdr3, mhc, pep = row[\"CDR3\"], row[\"MHC\"], row[\"Epitope\"]\n",
    "            \n",
    "            # Same-length different peptides\n",
    "            candidates = peps_by_len[len(pep)]\n",
    "            neg_peps = [p for p in candidates if p != pep]\n",
    "            \n",
    "            if len(neg_peps) >= k_neg:\n",
    "                selected_negs = random.sample(neg_peps, k_neg)\n",
    "            else:\n",
    "                selected_negs = neg_peps + [self._mutate_peptide(pep) for _ in range(k_neg - len(neg_peps))]\n",
    "            \n",
    "            for neg_pep in selected_negs:\n",
    "                negatives.append({\"CDR3\": cdr3, \"MHC\": mhc, \"Epitope\": neg_pep, \"label\": 0})\n",
    "            \n",
    "            # Different TCR for same peptide\n",
    "            other_tcrs = df[df[\"Epitope\"] != pep][\"CDR3\"].unique()\n",
    "            if len(other_tcrs) > 0:\n",
    "                neg_tcr = random.choice(other_tcrs)\n",
    "                negatives.append({\"CDR3\": neg_tcr, \"MHC\": mhc, \"Epitope\": pep, \"label\": 0})\n",
    "        \n",
    "        df_neg = pd.DataFrame(negatives)\n",
    "        return pd.concat([df, df_neg], ignore_index=True).drop_duplicates()\n",
    "    \n",
    "    def _mutate_peptide(self, peptide: str, n_mut: int = 1) -> str:\n",
    "        \"\"\"Mutate peptide for negative sampling\"\"\"\n",
    "        s = list(peptide)\n",
    "        for _ in range(n_mut):\n",
    "            pos = random.randint(0, len(s) - 1)\n",
    "            original = s[pos]\n",
    "            s[pos] = random.choice([aa for aa in AA_STANDARD if aa != original])\n",
    "        return \"\".join(s)\n",
    "    \n",
    "    def _split_data(self, df: pd.DataFrame):\n",
    "        \"\"\"Split data by epitope to avoid leakage\"\"\"\n",
    "        positive_epitopes = df[df[\"label\"] == 1][\"Epitope\"].unique()\n",
    "        random.shuffle(positive_epitopes)\n",
    "        \n",
    "        n_test = max(1, int(len(positive_epitopes) * self.config.test_size))\n",
    "        n_val = max(1, int(len(positive_epitopes) * self.config.val_size))\n",
    "        \n",
    "        test_epitopes = set(positive_epitopes[:n_test])\n",
    "        val_epitopes = set(positive_epitopes[n_test:n_test + n_val])\n",
    "        train_epitopes = set(positive_epitopes[n_test + n_val:])\n",
    "        \n",
    "        self.df_test = df[df[\"Epitope\"].isin(test_epitopes)].reset_index(drop=True)\n",
    "        self.df_val = df[df[\"Epitope\"].isin(val_epitopes)].reset_index(drop=True)\n",
    "        self.df_train = df[df[\"Epitope\"].isin(train_epitopes)].reset_index(drop=True)\n",
    "\n",
    "# --------------------------\n",
    "# Model Components\n",
    "# --------------------------\n",
    "\n",
    "class ESMSequenceEncoder:\n",
    "    def __init__(self, model_name: str, device: str, freeze_layers: int = 6):\n",
    "        self.device = device\n",
    "        self.tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "        self.model = EsmModel.from_pretrained(model_name).to(device)\n",
    "        \n",
    "        # Selective unfreezing\n",
    "        total_layers = len(self.model.encoder.layer)\n",
    "        for i, layer in enumerate(self.model.encoder.layer):\n",
    "            if i < freeze_layers:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            else:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "        \n",
    "        # Always fine-tune embeddings\n",
    "        for param in self.model.embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "        if hasattr(self.model, 'pooler') and self.model.pooler:\n",
    "            for param in self.model.pooler.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        self.hidden_size = self.model.config.hidden_size\n",
    "        \n",
    "    def encode_batch(self, sequences: List[str], max_length: int = 512) -> torch.Tensor:\n",
    "        clean_seqs = []\n",
    "        for seq in sequences:\n",
    "            clean_seq = \"\".join([c for c in seq.upper() if c in AA_SET])\n",
    "            clean_seqs.append(clean_seq if clean_seq else \"A\")\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            clean_seqs, return_tensors=\"pt\", padding=True, \n",
    "            truncation=True, max_length=max_length\n",
    "        ).to(self.device)\n",
    "        \n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "\n",
    "class AblationModel(nn.Module):\n",
    "    def __init__(self, esm_encoder: ESMSequenceEncoder, d_model: int = 128, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.esm_encoder = esm_encoder\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Projectors\n",
    "        self.proj_tcr = nn.Sequential(\n",
    "            nn.Linear(esm_encoder.hidden_size, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.proj_mhc = nn.Sequential(\n",
    "            nn.Linear(esm_encoder.hidden_size, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.proj_peptide = nn.Sequential(\n",
    "            nn.Linear(esm_encoder.hidden_size, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model * 3, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "        \n",
    "        # InfoNCE projector\n",
    "        self.infonce_proj = nn.Linear(d_model * 3, d_model)\n",
    "        \n",
    "    def forward(self, cdr3_seqs: List[str], mhc_alleles: List[str], epitope_seqs: List[str], mhc_encoding: str = \"pseudo_seq\"):\n",
    "        # Convert MHC alleles to sequences\n",
    "        mhc_seqs = [get_mhc_sequence(allele, mhc_encoding) for allele in mhc_alleles]\n",
    "        \n",
    "        # Encode\n",
    "        tcr_emb = self.esm_encoder.encode_batch(cdr3_seqs)\n",
    "        mhc_emb = self.esm_encoder.encode_batch(mhc_seqs)\n",
    "        pep_emb = self.esm_encoder.encode_batch(epitope_seqs)\n",
    "        \n",
    "        # Project\n",
    "        tcr_proj = self.proj_tcr(tcr_emb)\n",
    "        mhc_proj = self.proj_mhc(mhc_emb)\n",
    "        pep_proj = self.proj_peptide(pep_emb)\n",
    "        \n",
    "        # Concatenate\n",
    "        combined = torch.cat([tcr_proj, mhc_proj, pep_proj], dim=-1)\n",
    "        \n",
    "        # Classification logits\n",
    "        logits = self.classifier(combined).squeeze(-1)\n",
    "        \n",
    "        # InfoNCE features\n",
    "        infonce_features = self.infonce_proj(combined)\n",
    "        \n",
    "        return logits, infonce_features\n",
    "\n",
    "class InfoNCELoss(nn.Module):\n",
    "    def __init__(self, temperature: float = 0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, features: torch.Tensor, labels: torch.Tensor):\n",
    "        features = F.normalize(features, dim=1)\n",
    "        pos_mask = (labels == 1).float()\n",
    "        \n",
    "        if pos_mask.sum() < 2:\n",
    "            return torch.tensor(0.0, device=features.device)\n",
    "        \n",
    "        sim_matrix = torch.matmul(features, features.t()) / self.temperature\n",
    "        \n",
    "        pos_pairs = []\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i] == 1:\n",
    "                for j in range(i + 1, len(labels)):\n",
    "                    if labels[j] == 1:\n",
    "                        pos_pairs.append((i, j))\n",
    "        \n",
    "        if not pos_pairs:\n",
    "            return torch.tensor(0.0, device=features.device)\n",
    "        \n",
    "        loss = 0\n",
    "        for i, j in pos_pairs:\n",
    "            numerator = torch.exp(sim_matrix[i, j])\n",
    "            denominator = torch.sum(torch.exp(sim_matrix[i, :]))\n",
    "            loss += -torch.log(numerator / (denominator + 1e-8))\n",
    "        \n",
    "        return loss / len(pos_pairs)\n",
    "\n",
    "class TemperatureScaling:\n",
    "    def __init__(self):\n",
    "        self.temperature = 1.0\n",
    "    \n",
    "    def fit(self, logits: np.ndarray, labels: np.ndarray):\n",
    "        try:\n",
    "            from scipy.optimize import minimize_scalar\n",
    "            \n",
    "            def nll(temp):\n",
    "                if temp <= 0:\n",
    "                    return 1e6\n",
    "                scaled_logits = logits / temp\n",
    "                probs = 1 / (1 + np.exp(-scaled_logits))\n",
    "                probs = np.clip(probs, 1e-7, 1 - 1e-7)\n",
    "                return -np.mean(labels * np.log(probs) + (1 - labels) * np.log(1 - probs))\n",
    "            \n",
    "            result = minimize_scalar(nll, bounds=(0.1, 10.0), method='bounded')\n",
    "            self.temperature = result.x\n",
    "        except:\n",
    "            self.temperature = 1.0\n",
    "    \n",
    "    def apply(self, logits: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-logits / self.temperature))\n",
    "\n",
    "class PlattScaling:\n",
    "    def __init__(self):\n",
    "        self.classifier = None\n",
    "    \n",
    "    def fit(self, logits: np.ndarray, labels: np.ndarray):\n",
    "        self.classifier = LogisticRegression()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "            self.classifier.fit(logits.reshape(-1, 1), labels)\n",
    "    \n",
    "    def apply(self, logits: np.ndarray) -> np.ndarray:\n",
    "        if self.classifier is None:\n",
    "            return 1 / (1 + np.exp(-logits))\n",
    "        return self.classifier.predict_proba(logits.reshape(-1, 1))[:, 1]\n",
    "\n",
    "# --------------------------\n",
    "# Dataset\n",
    "# --------------------------\n",
    "\n",
    "class TCRDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return {\n",
    "            \"cdr3\": str(row[\"CDR3\"]),\n",
    "            \"mhc\": str(row[\"MHC\"]),\n",
    "            \"epitope\": str(row[\"Epitope\"]),\n",
    "            \"label\": float(row[\"label\"])\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"cdr3\": [item[\"cdr3\"] for item in batch],\n",
    "        \"mhc\": [item[\"mhc\"] for item in batch],\n",
    "        \"epitope\": [item[\"epitope\"] for item in batch],\n",
    "        \"labels\": torch.tensor([item[\"label\"] for item in batch], dtype=torch.float32)\n",
    "    }\n",
    "\n",
    "# --------------------------\n",
    "# Ablation Experiment Runner\n",
    "# --------------------------\n",
    "\n",
    "class AblationExperiment:\n",
    "    def __init__(self, config: AblationConfig):\n",
    "        self.config = config\n",
    "        self.data_manager = AblationDataManager(config)\n",
    "        \n",
    "    def run_single_experiment(self, freeze_layers: int, infonce_weight: float, d_model: int, \n",
    "                            calibration: str, neg_ratio: int, mhc_encoding: str) -> Dict[str, float]:\n",
    "        \"\"\"Run single experiment with specified parameters\"\"\"\n",
    "        \n",
    "        # Load data with specified negative ratio\n",
    "        self.data_manager.load_and_split_data(neg_ratio=neg_ratio)\n",
    "        \n",
    "        # Initialize model\n",
    "        esm_encoder = ESMSequenceEncoder(\n",
    "            self.config.esm_model_name, \n",
    "            self.config.device,\n",
    "            freeze_layers\n",
    "        )\n",
    "        \n",
    "        model = AblationModel(esm_encoder, d_model).to(self.config.device)\n",
    "        \n",
    "        # Training setup\n",
    "        esm_params = []\n",
    "        proj_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'esm_encoder' in name:\n",
    "                esm_params.append(param)\n",
    "            else:\n",
    "                proj_params.append(param)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': esm_params, 'lr': self.config.base_lr * 0.1},\n",
    "            {'params': proj_params, 'lr': self.config.base_lr}\n",
    "        ], weight_decay=0.01)\n",
    "        \n",
    "        bce_loss = nn.BCEWithLogitsLoss()\n",
    "        infonce_loss = InfoNCELoss()\n",
    "        \n",
    "        # Data loaders\n",
    "        train_dataset = TCRDataset(self.data_manager.df_train)\n",
    "        val_dataset = TCRDataset(self.data_manager.df_val)\n",
    "        test_dataset = TCRDataset(self.data_manager.df_test)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.config.batch_size, \n",
    "                                shuffle=True, collate_fn=collate_fn)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=self.config.batch_size, \n",
    "                              shuffle=False, collate_fn=collate_fn)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.config.batch_size, \n",
    "                               shuffle=False, collate_fn=collate_fn)\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for epoch in range(self.config.epochs):\n",
    "            for batch in train_loader:\n",
    "                labels = batch[\"labels\"].to(self.config.device)\n",
    "                \n",
    "                logits, features = model(batch[\"cdr3\"], batch[\"mhc\"], batch[\"epitope\"], mhc_encoding)\n",
    "                \n",
    "                # Combined loss\n",
    "                bce = bce_loss(logits, labels)\n",
    "                info_nce = infonce_loss(features, labels) if infonce_weight > 0 else torch.tensor(0.0)\n",
    "                loss = bce + infonce_weight * info_nce\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Calibration\n",
    "        model.eval()\n",
    "        val_logits = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                labels = batch[\"labels\"].to(self.config.device)\n",
    "                logits, _ = model(batch[\"cdr3\"], batch[\"mhc\"], batch[\"epitope\"], mhc_encoding)\n",
    "                \n",
    "                val_logits.extend(logits.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_logits = np.array(val_logits)\n",
    "        val_labels = np.array(val_labels)\n",
    "        \n",
    "        if calibration == \"temperature\":\n",
    "            calibrator = TemperatureScaling()\n",
    "            calibrator.fit(val_logits, val_labels)\n",
    "        elif calibration == \"platt\":\n",
    "            calibrator = PlattScaling()\n",
    "            calibrator.fit(val_logits, val_labels)\n",
    "        else:  # none\n",
    "            calibrator = None\n",
    "        \n",
    "        # Test evaluation\n",
    "        test_logits = []\n",
    "        test_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                labels = batch[\"labels\"].to(self.config.device)\n",
    "                logits, _ = model(batch[\"cdr3\"], batch[\"mhc\"], batch[\"epitope\"], mhc_encoding)\n",
    "                \n",
    "                test_logits.extend(logits.cpu().numpy())\n",
    "                test_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        test_logits = np.array(test_logits)\n",
    "        test_labels = np.array(test_labels)\n",
    "        \n",
    "        # Apply calibration\n",
    "        if calibrator:\n",
    "            test_probs = calibrator.apply(test_logits)\n",
    "        else:\n",
    "            test_probs = 1 / (1 + np.exp(-test_logits))\n",
    "        \n",
    "        test_preds = (test_probs > 0.5).astype(int)\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            \"auc\": roc_auc_score(test_labels, test_probs),\n",
    "            \"auprc\": average_precision_score(test_labels, test_probs),\n",
    "            \"accuracy\": accuracy_score(test_labels, test_preds),\n",
    "            \"precision\": ((test_preds == 1) & (test_labels == 1)).sum() / max(1, (test_preds == 1).sum()),\n",
    "            \"recall\": ((test_preds == 1) & (test_labels == 1)).sum() / max(1, (test_labels == 1).sum()),\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def run_ablation_studies(self) -> pd.DataFrame:\n",
    "        \"\"\"Run comprehensive ablation studies\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # 1. ESM Freezing Strategy Ablation\n",
    "        print(\"=\" * 60)\n",
    "        print(\"1. ESM Freezing Strategy Ablation\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        base_params = {\n",
    "            \"infonce_weight\": 0.1,\n",
    "            \"d_model\": 128,\n",
    "            \"calibration\": \"temperature\",\n",
    "            \"neg_ratio\": 4,\n",
    "            \"mhc_encoding\": \"pseudo_seq\"\n",
    "        }\n",
    "        \n",
    "        for freeze_layers in self.config.freeze_layers_options:\n",
    "            print(f\"Testing freeze_layers = {freeze_layers}\")\n",
    "            \n",
    "            run_metrics = []\n",
    "            for run in range(self.config.n_runs):\n",
    "                set_seed(self.config.seed + run)\n",
    "                metrics = self.run_single_experiment(freeze_layers=freeze_layers, **base_params)\n",
    "                run_metrics.append(metrics)\n",
    "            \n",
    "            # Aggregate metrics\n",
    "            for metric_name in run_metrics[0].keys():\n",
    "                values = [m[metric_name] for m in run_metrics]\n",
    "                mean_val = np.mean(values)\n",
    "                std_val = np.std(values)\n",
    "                \n",
    "                results.append({\n",
    "                    \"ablation_type\": \"freeze_layers\",\n",
    "                    \"parameter\": f\"freeze_{freeze_layers}\",\n",
    "                    \"metric\": metric_name,\n",
    "                    \"mean\": mean_val,\n",
    "                    \"std\": std_val,\n",
    "                    \"ci_string\": f\"{mean_val:.4f} ± {1.96 * std_val / np.sqrt(self.config.n_runs):.4f}\"\n",
    "                })\n",
    "        \n",
    "        # 2. InfoNCE Weight Ablation\n",
    "        print(\"=\" * 60)\n",
    "        print(\"2. InfoNCE Weight Ablation\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        base_params[\"freeze_layers\"] = 6  # Reset to best from previous ablation\n",
    "        \n",
    "        for infonce_weight in self.config.infonce_weight_options:\n",
    "            print(f\"Testing infonce_weight = {infonce_weight}\")\n",
    "            \n",
    "            run_metrics = []\n",
    "            for run in range(self.config.n_runs):\n",
    "                set_seed(self.config.seed + run)\n",
    "                params = base_params.copy()\n",
    "                params[\"infonce_weight\"] = infonce_weight\n",
    "                metrics = self.run_single_experiment(**params)\n",
    "                run_metrics.append(metrics)\n",
    "            \n",
    "            # Aggregate metrics\n",
    "            for metric_name in run_metrics[0].keys():\n",
    "                values = [m[metric_name] for m in run_metrics]\n",
    "                mean_val = np.mean(values)\n",
    "                std_val = np.std(values)\n",
    "                \n",
    "                results.append({\n",
    "                    \"ablation_type\": \"infonce_weight\",\n",
    "                    \"parameter\": f\"weight_{infonce_weight}\",\n",
    "                    \"metric\": metric_name,\n",
    "                    \"mean\": mean_val,\n",
    "                    \"std\": std_val,\n",
    "                    \"ci_string\": f\"{mean_val:.4f} ± {1.96 * std_val / np.sqrt(self.config.n_runs):.4f}\"\n",
    "                })\n",
    "        \n",
    "        # 3. Model Dimension Ablation\n",
    "        print(\"=\" * 60)\n",
    "        print(\"3. Model Dimension Ablation\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for d_model in self.config.d_model_options:\n",
    "            print(f\"Testing d_model = {d_model}\")\n",
    "            \n",
    "            run_metrics = []\n",
    "            for run in range(self.config.n_runs):\n",
    "                set_seed(self.config.seed + run)\n",
    "                params = base_params.copy()\n",
    "                params[\"d_model\"] = d_model\n",
    "                metrics = self.run_single_experiment(**params)\n",
    "                run_metrics.append(metrics)\n",
    "            \n",
    "            # Aggregate metrics\n",
    "            for metric_name in run_metrics[0].keys():\n",
    "                values = [m[metric_name] for m in run_metrics]\n",
    "                mean_val = np.mean(values)\n",
    "                std_val = np.std(values)\n",
    "                \n",
    "                results.append({\n",
    "                    \"ablation_type\": \"d_model\",\n",
    "                    \"parameter\": f\"dim_{d_model}\",\n",
    "                    \"metric\": metric_name,\n",
    "                    \"mean\": mean_val,\n",
    "                    \"std\": std_val,\n",
    "                    \"ci_string\": f\"{mean_val:.4f} ± {1.96 * std_val / np.sqrt(self.config.n_runs):.4f}\"\n",
    "                })\n",
    "        \n",
    "        # 4. Calibration Method Ablation\n",
    "        print(\"=\" * 60)\n",
    "        print(\"4. Calibration Method Ablation\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for calibration in self.config.calibration_options:\n",
    "            print(f\"Testing calibration = {calibration}\")\n",
    "            \n",
    "            run_metrics = []\n",
    "            for run in range(self.config.n_runs):\n",
    "                set_seed(self.config.seed + run)\n",
    "                params = base_params.copy()\n",
    "                params[\"calibration\"] = calibration\n",
    "                metrics = self.run_single_experiment(**params)\n",
    "                run_metrics.append(metrics)\n",
    "            \n",
    "            # Aggregate metrics\n",
    "            for metric_name in run_metrics[0].keys():\n",
    "                values = [m[metric_name] for m in run_metrics]\n",
    "                mean_val = np.mean(values)\n",
    "                std_val = np.std(values)\n",
    "                \n",
    "                results.append({\n",
    "                    \"ablation_type\": \"calibration\",\n",
    "                    \"parameter\": f\"calib_{calibration}\",\n",
    "                    \"metric\": metric_name,\n",
    "                    \"mean\": mean_val,\n",
    "                    \"std\": std_val,\n",
    "                    \"ci_string\": f\"{mean_val:.4f} ± {1.96 * std_val / np.sqrt(self.config.n_runs):.4f}\"\n",
    "                })\n",
    "        \n",
    "        # 5. Negative Sampling Ratio Ablation\n",
    "        print(\"=\" * 60)\n",
    "        print(\"5. Negative Sampling Ratio Ablation\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for neg_ratio in self.config.neg_ratio_options:\n",
    "            print(f\"Testing neg_ratio = {neg_ratio}\")\n",
    "            \n",
    "            run_metrics = []\n",
    "            for run in range(self.config.n_runs):\n",
    "                set_seed(self.config.seed + run)\n",
    "                params = base_params.copy()\n",
    "                params[\"neg_ratio\"] = neg_ratio\n",
    "                metrics = self.run_single_experiment(**params)\n",
    "                run_metrics.append(metrics)\n",
    "            \n",
    "            # Aggregate metrics\n",
    "            for metric_name in run_metrics[0].keys():\n",
    "                values = [m[metric_name] for m in run_metrics]\n",
    "                mean_val = np.mean(values)\n",
    "                std_val = np.std(values)\n",
    "                \n",
    "                results.append({\n",
    "                    \"ablation_type\": \"neg_ratio\",\n",
    "                    \"parameter\": f\"ratio_{neg_ratio}\",\n",
    "                    \"metric\": metric_name,\n",
    "                    \"mean\": mean_val,\n",
    "                    \"std\": std_val,\n",
    "                    \"ci_string\": f\"{mean_val:.4f} ± {1.96 * std_val / np.sqrt(self.config.n_runs):.4f}\"\n",
    "                })\n",
    "        \n",
    "        # 6. MHC Encoding Ablation\n",
    "        print(\"=\" * 60)\n",
    "        print(\"6. MHC Encoding Ablation\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for mhc_encoding in self.config.mhc_encoding_options:\n",
    "            print(f\"Testing mhc_encoding = {mhc_encoding}\")\n",
    "            \n",
    "            run_metrics = []\n",
    "            for run in range(self.config.n_runs):\n",
    "                set_seed(self.config.seed + run)\n",
    "                params = base_params.copy()\n",
    "                params[\"mhc_encoding\"] = mhc_encoding\n",
    "                metrics = self.run_single_experiment(**params)\n",
    "                run_metrics.append(metrics)\n",
    "            \n",
    "            # Aggregate metrics\n",
    "            for metric_name in run_metrics[0].keys():\n",
    "                values = [m[metric_name] for m in run_metrics]\n",
    "                mean_val = np.mean(values)\n",
    "                std_val = np.std(values)\n",
    "                \n",
    "                results.append({\n",
    "                    \"ablation_type\": \"mhc_encoding\",\n",
    "                    \"parameter\": f\"enc_{mhc_encoding}\",\n",
    "                    \"metric\": metric_name,\n",
    "                    \"mean\": mean_val,\n",
    "                    \"std\": std_val,\n",
    "                    \"ci_string\": f\"{mean_val:.4f} ± {1.96 * std_val / np.sqrt(self.config.n_runs):.4f}\"\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def save_results(self, results_df: pd.DataFrame, output_path: str = \"ablation_results.csv\"):\n",
    "        \"\"\"Save results to CSV and print summary\"\"\"\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\n{'='*120}\")\n",
    "        print(\"ABLATION STUDY RESULTS SUMMARY\")\n",
    "        print(f\"{'='*120}\")\n",
    "        \n",
    "        # Show results by ablation type\n",
    "        for ablation_type in results_df[\"ablation_type\"].unique():\n",
    "            print(f\"\\n{ablation_type.upper()} ABLATION:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            subset = results_df[results_df[\"ablation_type\"] == ablation_type]\n",
    "            pivot = subset.pivot(index=\"parameter\", columns=\"metric\", values=\"ci_string\")\n",
    "            \n",
    "            # Reorder columns\n",
    "            metric_order = [\"auc\", \"auprc\", \"accuracy\", \"precision\", \"recall\"]\n",
    "            available_metrics = [col for col in metric_order if col in pivot.columns]\n",
    "            pivot = pivot[available_metrics]\n",
    "            \n",
    "            print(pivot.to_string())\n",
    "        \n",
    "        print(f\"\\nDetailed results saved to: {output_path}\")\n",
    "        print(f\"{'='*120}\")\n",
    "\n",
    "# --------------------------\n",
    "# Main Function\n",
    "# --------------------------\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main ablation study runner\"\"\"\n",
    "    config = AblationConfig(\n",
    "        seed=42,\n",
    "        n_runs=3,\n",
    "        epochs=10,  # Reduced for faster ablation\n",
    "        batch_size=64,\n",
    "        base_lr=1e-4\n",
    "    )\n",
    "    \n",
    "    print(\"Starting Ablation Study for TCR-peptide-MHC Binding Prediction\")\n",
    "    print(f\"Device: {config.device}\")\n",
    "    print(f\"Configuration: {config.n_runs} runs, {config.epochs} epochs per run\")\n",
    "    \n",
    "    # Run ablation studies\n",
    "    experiment = AblationExperiment(config)\n",
    "    results_df = experiment.run_ablation_studies()\n",
    "    \n",
    "    # Save and display results\n",
    "    experiment.save_results(results_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8dec5-9ff2-4d22-96c1-2545e4db7f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
