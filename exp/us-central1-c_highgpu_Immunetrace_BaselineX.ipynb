{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a73d7a-cc29-455f-987d-4dc0027a2a82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 14:18:10.964036: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-20 14:18:11.016180: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive TCR-peptide-MHC binding prediction benchmark...\n",
      "Device: cuda\n",
      "Configuration: epochs=8, batch_size=32, lr=0.0002\n",
      "Loading and preparing data...\n",
      "Data split: Train=20162, Val=2608, Test=6593\n",
      "\n",
      "============================================================\n",
      "Running B1_ESM_InfoNCE_TempScale...\n",
      "============================================================\n",
      "Run 1/3\n",
      "[Info] Loading ESM model: facebook/esm2_t12_35M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] ESM loaded, hidden_size: 480, trainable layers: 6\n",
      "  AUC: 0.6428, AUPRC: 0.2744, Acc: 0.7767\n",
      "Run 2/3\n",
      "[Info] Loading ESM model: facebook/esm2_t12_35M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] ESM loaded, hidden_size: 480, trainable layers: 6\n",
      "  AUC: 0.6400, AUPRC: 0.2866, Acc: 0.7767\n",
      "Run 3/3\n",
      "[Info] Loading ESM model: facebook/esm2_t12_35M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] ESM loaded, hidden_size: 480, trainable layers: 6\n",
      "  AUC: 0.6156, AUPRC: 0.2886, Acc: 0.7767\n",
      "\n",
      "============================================================\n",
      "Running B2_Simple_Features_LR...\n",
      "============================================================\n",
      "Run 1/3\n",
      "  AUC: 0.5086, AUPRC: 0.2108, Acc: 0.7767\n",
      "Run 2/3\n",
      "  AUC: 0.5086, AUPRC: 0.2108, Acc: 0.7767\n",
      "Run 3/3\n",
      "  AUC: 0.5086, AUPRC: 0.2108, Acc: 0.7767\n",
      "\n",
      "============================================================\n",
      "Running B3_Simple_Features_RF...\n",
      "============================================================\n",
      "Run 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_13941/886007612.py:630: RuntimeWarning: divide by zero encountered in log\n",
      "  val_logits = np.log(val_probs / (1 - val_probs + 1e-8))\n",
      "/var/tmp/ipykernel_13941/886007612.py:668: RuntimeWarning: divide by zero encountered in log\n",
      "  logits = np.log(probs / (1 - probs + 1e-8))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AUC: 0.5373, AUPRC: 0.2556, Acc: 0.7767\n",
      "Run 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_13941/886007612.py:630: RuntimeWarning: divide by zero encountered in log\n",
      "  val_logits = np.log(val_probs / (1 - val_probs + 1e-8))\n",
      "/var/tmp/ipykernel_13941/886007612.py:668: RuntimeWarning: divide by zero encountered in log\n",
      "  logits = np.log(probs / (1 - probs + 1e-8))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AUC: 0.5373, AUPRC: 0.2556, Acc: 0.7767\n",
      "Run 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_13941/886007612.py:630: RuntimeWarning: divide by zero encountered in log\n",
      "  val_logits = np.log(val_probs / (1 - val_probs + 1e-8))\n",
      "/var/tmp/ipykernel_13941/886007612.py:668: RuntimeWarning: divide by zero encountered in log\n",
      "  logits = np.log(probs / (1 - probs + 1e-8))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AUC: 0.5373, AUPRC: 0.2556, Acc: 0.7767\n",
      "\n",
      "============================================================\n",
      "Running B4_Simple_Features_SVM...\n",
      "============================================================\n",
      "Run 1/3\n",
      "  AUC: 0.5829, AUPRC: 0.2685, Acc: 0.7767\n",
      "Run 2/3\n",
      "  AUC: 0.5829, AUPRC: 0.2685, Acc: 0.7767\n",
      "Run 3/3\n",
      "  AUC: 0.5829, AUPRC: 0.2685, Acc: 0.7767\n",
      "\n",
      "============================================================\n",
      "Running BX1_ProtBert_Like...\n",
      "============================================================\n",
      "Run 1/3\n",
      "  AUC: 0.4028, AUPRC: 0.1767, Acc: 0.7767\n",
      "Run 2/3\n",
      "  AUC: 0.4028, AUPRC: 0.1767, Acc: 0.7767\n",
      "Run 3/3\n",
      "  AUC: 0.4028, AUPRC: 0.1767, Acc: 0.7767\n",
      "\n",
      "============================================================\n",
      "Running BX2_Simple_Graph...\n",
      "============================================================\n",
      "Run 1/3\n",
      "  AUC: 0.4628, AUPRC: 0.1955, Acc: 0.7702\n",
      "Run 2/3\n",
      "  AUC: 0.4628, AUPRC: 0.1955, Acc: 0.7702\n",
      "\n",
      "============================================================\n",
      "Running BX3_VAE_Like...\n",
      "============================================================\n",
      "Run 1/3\n",
      "  AUC: 0.5593, AUPRC: 0.2704, Acc: 0.7767\n",
      "Run 2/3\n",
      "  AUC: 0.5593, AUPRC: 0.2704, Acc: 0.7767\n",
      "Run 3/3\n",
      "  AUC: 0.5593, AUPRC: 0.2704, Acc: 0.7767\n",
      "\n",
      "========================================================================================================================\n",
      "COMPREHENSIVE TCR-PEPTIDE-MHC BINDING PREDICTION BENCHMARK\n",
      "========================================================================================================================\n",
      "Metric                                AUC            AUPRC         ACCURACY        PRECISION           RECALL\n",
      "Model                                                                                                        \n",
      "B1_ESM_InfoNCE_TempScale  0.6328 ± 0.0138  0.2832 ± 0.0071  0.7767 ± 0.0000  0.0000 ± 0.0000  0.0000 ± 0.0000\n",
      "B2_Simple_Features_LR     0.5086 ± 0.0000  0.2108 ± 0.0000  0.7767 ± 0.0000  0.0000 ± 0.0000  0.0000 ± 0.0000\n",
      "B3_Simple_Features_RF     0.5373 ± 0.0000  0.2556 ± 0.0000  0.7767 ± 0.0000  0.5000 ± 0.0000  0.0007 ± 0.0000\n",
      "B4_Simple_Features_SVM    0.5829 ± 0.0000  0.2685 ± 0.0000  0.7767 ± 0.0000  0.0000 ± 0.0000  0.0000 ± 0.0000\n",
      "BX1_ProtBert_Like         0.4028 ± 0.0000  0.1767 ± 0.0000  0.7767 ± 0.0000  0.0000 ± 0.0000  0.0000 ± 0.0000\n",
      "BX2_Simple_Graph          0.4628 ± 0.0000  0.1955 ± 0.0000  0.7702 ± 0.0000  0.2471 ± 0.0000  0.0143 ± 0.0000\n",
      "BX3_VAE_Like              0.5593 ± 0.0000  0.2704 ± 0.0000  0.7767 ± 0.0000  0.0000 ± 0.0000  0.0000 ± 0.0000\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE HIGHLIGHTS:\n",
      "============================================================\n",
      "Top 3 Models by AUC:\n",
      "1. B1_ESM_InfoNCE_TempScale: 0.6328 ± 0.0138\n",
      "2. B4_Simple_Features_SVM: 0.5829 ± 0.0000\n",
      "3. BX3_VAE_Like: 0.5593 ± 0.0000\n",
      "\n",
      "Detailed results saved to: comprehensive_results.csv\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Comprehensive TCR-peptide-MHC Binding Prediction Benchmark\n",
    "# Main Table + Auxiliary Methods (BX series)\n",
    "# Dependencies: torch, transformers, numpy, pandas, scikit-learn, tqdm, requests\n",
    "\n",
    "import os, sys, math, json, time, random, requests, warnings, hashlib\n",
    "from typing import List, Dict, Tuple, Set, Optional, Any, Union\n",
    "from collections import defaultdict, Counter\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import EsmModel, EsmTokenizer, BertModel, BertTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please install tqdm: pip install tqdm\")\n",
    "\n",
    "# --------------------------\n",
    "# Configuration & Utils\n",
    "# --------------------------\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    n_runs: int = 3\n",
    "    test_size: float = 0.2\n",
    "    val_size: float = 0.1\n",
    "    batch_size: int = 64\n",
    "    epochs: int = 10\n",
    "    lr: float = 1e-4\n",
    "    \n",
    "    # ESM config\n",
    "    esm_model_name: str = \"facebook/esm2_t12_35M_UR50D\"\n",
    "    esm_freeze_layers: int = 6\n",
    "    d_model: int = 128\n",
    "    \n",
    "    # ProtBert config\n",
    "    protbert_model_name: str = \"Rostlab/prot_bert\"\n",
    "    protbert_freeze_layers: int = 8\n",
    "    \n",
    "    # k-mer config\n",
    "    kmer_range: Tuple[int, int] = field(default_factory=lambda: (3, 5))\n",
    "    hash_features: int = 100000\n",
    "    \n",
    "    # CNN config\n",
    "    cnn_channels: List[int] = field(default_factory=lambda: [32, 64, 128])\n",
    "    cnn_kernel_sizes: List[int] = field(default_factory=lambda: [3, 5, 7])\n",
    "    \n",
    "    # LSTM config\n",
    "    lstm_hidden: int = 128\n",
    "    lstm_layers: int = 2\n",
    "    \n",
    "    # Transformer config\n",
    "    transformer_layers: int = 4\n",
    "    transformer_heads: int = 8\n",
    "    \n",
    "    # GNN config\n",
    "    gnn_hidden: int = 128\n",
    "    gnn_layers: int = 2\n",
    "    \n",
    "    # VAE config\n",
    "    vae_latent_dim: int = 64\n",
    "    \n",
    "    # Retrieval config\n",
    "    retrieval_top_k: int = 200\n",
    "    \n",
    "    # TCR distance config\n",
    "    tcr_knn_k: List[int] = field(default_factory=lambda: [1, 5, 10])\n",
    "\n",
    "AA_STANDARD = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "AA_SET = set(AA_STANDARD)\n",
    "AA_TO_IDX = {aa: i for i, aa in enumerate(AA_STANDARD)}\n",
    "\n",
    "# MHC pseudo-sequences\n",
    "MHC_PSEUDO_SEQUENCES = {\n",
    "    \"HLA-A*02:01\": \"GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETLQRTDAPKTHMTHHAVSDHEATLRCWALSFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGQEQRYTCHVQHEGLPKPLTLRWE\",\n",
    "    \"HLA-A*01:01\": \"GSHSMRYFFTSVSRPGRGEPRFIAMGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDRETQKAKGNEQSFRVDLRTLLGYYNQSEDGSHTIQIMYGCDVGPDGRLLRGYDQYAYDGKDYIALNEDLRSWTAADTAAQITQRKWEAARVAEQLRAYLEGTCVEWLRRYLENGKDKLERADPPKTHVTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWE\",\n",
    "    \"HLA-B*07:02\": \"GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDRETQKAKGNEQSFRVDLRTLLGYYNQSEDGSHTIQIMYGCDVGPDGRLLRGYDQYAYDGKDYIALNEDLRSWTAADTAAQITQRKWEAARVAEQLRAYLEGTCVEWLRRYLENGKDKLERADPPKTHVTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWE\"\n",
    "}\n",
    "\n",
    "def get_mhc_sequence(mhc_name: str) -> str:\n",
    "    return MHC_PSEUDO_SEQUENCES.get(mhc_name, MHC_PSEUDO_SEQUENCES[\"HLA-A*02:01\"])\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def aa_to_indices(sequence: str, max_len: int = 50) -> np.ndarray:\n",
    "    \"\"\"Convert amino acid sequence to indices\"\"\"\n",
    "    seq = sequence.upper()[:max_len]\n",
    "    indices = np.zeros(max_len, dtype=np.long)\n",
    "    for i, aa in enumerate(seq):\n",
    "        if aa in AA_TO_IDX:\n",
    "            indices[i] = AA_TO_IDX[aa]\n",
    "    return indices\n",
    "\n",
    "def compute_sequence_similarity(seq1: str, seq2: str) -> float:\n",
    "    \"\"\"Compute sequence similarity using simple alignment\"\"\"\n",
    "    from difflib import SequenceMatcher\n",
    "    return SequenceMatcher(None, seq1, seq2).ratio()\n",
    "\n",
    "# --------------------------\n",
    "# Data Management (Enhanced)\n",
    "# --------------------------\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        self.config = config\n",
    "        self.df_train = None\n",
    "        self.df_val = None\n",
    "        self.df_test = None\n",
    "        \n",
    "    def load_and_split_data(self, data_path: str = \"data.csv\"):\n",
    "        \"\"\"Load data and create train/val/test splits\"\"\"\n",
    "        if not os.path.exists(data_path):\n",
    "            self._create_enhanced_demo_data(data_path)\n",
    "        \n",
    "        df = pd.read_csv(data_path)\n",
    "        df = self._preprocess_data(df)\n",
    "        df_with_negs = self._build_negatives(df)\n",
    "        self._split_data(df_with_negs)\n",
    "        \n",
    "        print(f\"Data split: Train={len(self.df_train)}, Val={len(self.df_val)}, Test={len(self.df_test)}\")\n",
    "        \n",
    "    def _create_enhanced_demo_data(self, path: str):\n",
    "        \"\"\"Create enhanced demonstration dataset\"\"\"\n",
    "        demo_data = [\n",
    "            [\"CDR3\", \"MHC\", \"Epitope\"],\n",
    "            # HLA-A*02:01 epitopes\n",
    "            [\"CASSLEETQYF\", \"HLA-A*02:01\", \"GILGFVFTL\"],\n",
    "            [\"CASSFRGTQYF\", \"HLA-A*02:01\", \"NLVPMVATV\"],\n",
    "            [\"CASRPGLAGGRPEQYF\", \"HLA-A*02:01\", \"TPRVTGGGAM\"],\n",
    "            [\"CSVEGGSTDTQYF\", \"HLA-A*02:01\", \"ELAGIGILTV\"],\n",
    "            [\"CASSQDTQYF\", \"HLA-A*02:01\", \"LLWNGPMAV\"],\n",
    "            [\"CAWRNTGQLYF\", \"HLA-A*02:01\", \"KLVALGINAV\"],\n",
    "            [\"CASTLESGQYF\", \"HLA-A*02:01\", \"VTEHDTLLY\"],\n",
    "            [\"CASSPPRVYNEQFF\", \"HLA-A*02:01\", \"LLWNGPMAV\"],\n",
    "            [\"CASSPGQGAYNEQFF\", \"HLA-A*02:01\", \"GILGFVFTL\"],\n",
    "            [\"CASSRGQGVYNEQFF\", \"HLA-A*02:01\", \"FLKEKGGL\"],\n",
    "            [\"CASSPRGTDTQYF\", \"HLA-A*02:01\", \"IMDQVPFSV\"],\n",
    "            [\"CASSFDRVGDNEQFF\", \"HLA-A*02:01\", \"KLGGALQAK\"],\n",
    "            [\"CASSLVGAGGRPEQYF\", \"HLA-A*02:01\", \"GVYDGREHTV\"],\n",
    "            [\"CASSITGQGDNEQFF\", \"HLA-A*02:01\", \"YLQPRTFLL\"],\n",
    "            [\"CASSFGQGAYNEQFF\", \"HLA-A*02:01\", \"ALWEIQQVV\"],\n",
    "            # HLA-A*01:01 epitopes\n",
    "            [\"CASSLEETQYF\", \"HLA-A*01:01\", \"TTPESANL\"],\n",
    "            [\"CASSQVGQGAYNEQFF\", \"HLA-A*01:01\", \"IVDCLTEMY\"],\n",
    "            [\"CASSRGDTQYF\", \"HLA-A*01:01\", \"VTEHDTLLY\"],\n",
    "            [\"CASSPPGQGAYNEQFF\", \"HLA-A*01:01\", \"TTPESANL\"],\n",
    "            [\"CASSLVGAYNEQFF\", \"HLA-A*01:01\", \"IVDCLTEMY\"],\n",
    "            # HLA-B*07:02 epitopes  \n",
    "            [\"CASSFRGTQYF\", \"HLA-B*07:02\", \"APRTLVYLL\"],\n",
    "            [\"CASSLGQGAVGEQFF\", \"HLA-B*07:02\", \"FPVRPQVPL\"],\n",
    "            [\"CASSRGDNEQFF\", \"HLA-B*07:02\", \"APRTLVYLL\"],\n",
    "            [\"CASSPPGAYNEQFF\", \"HLA-B*07:02\", \"FPVRPQVPL\"],\n",
    "            [\"CASSLVGNEQFF\", \"HLA-B*07:02\", \"GPRLGVRAT\"],\n",
    "            # Additional diverse examples\n",
    "            [\"CASSYDRGDTQYF\", \"HLA-A*02:01\", \"GLCTLVAML\"],\n",
    "            [\"CASSQGQGAYNEQFF\", \"HLA-A*02:01\", \"RLRAEAQVK\"],\n",
    "            [\"CASSLGDTQYF\", \"HLA-A*01:01\", \"ASNENMETM\"],\n",
    "            [\"CASSPRNEQFF\", \"HLA-B*07:02\", \"RPRGEVRFL\"],\n",
    "            [\"CASSRGQGDTQYF\", \"HLA-A*02:01\", \"FRDYVDRFYKTLRAEQASQE\"],  # Longer epitope\n",
    "        ]\n",
    "        \n",
    "        with open(path, \"w\") as f:\n",
    "            for row in demo_data:\n",
    "                f.write(\",\".join(row) + \"\\n\")\n",
    "    \n",
    "    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and preprocess data\"\"\"\n",
    "        df = df.copy()\n",
    "        df[\"CDR3\"] = df[\"CDR3\"].astype(str).str.upper()\n",
    "        df[\"MHC\"] = df[\"MHC\"].astype(str).str.upper()\n",
    "        df[\"Epitope\"] = df[\"Epitope\"].astype(str).str.upper()\n",
    "        df = df.dropna().drop_duplicates()\n",
    "        df = df[df[\"Epitope\"].map(lambda p: all(ch in AA_SET for ch in p))]\n",
    "        df = df[df[\"CDR3\"].map(lambda p: all(ch in AA_SET for ch in p))]\n",
    "        return df\n",
    "    \n",
    "    def _build_negatives(self, df: pd.DataFrame, k_neg: int = 4) -> pd.DataFrame:\n",
    "        \"\"\"Generate negative samples\"\"\"\n",
    "        df = df.copy()\n",
    "        df[\"label\"] = 1\n",
    "        \n",
    "        # Group peptides by length for realistic negatives\n",
    "        peps_by_len = defaultdict(list)\n",
    "        for pep in df[\"Epitope\"].unique():\n",
    "            peps_by_len[len(pep)].append(pep)\n",
    "        \n",
    "        negatives = []\n",
    "        for _, row in df.iterrows():\n",
    "            cdr3, mhc, pep = row[\"CDR3\"], row[\"MHC\"], row[\"Epitope\"]\n",
    "            \n",
    "            # Strategy 1: Same-length different peptides\n",
    "            candidates = peps_by_len[len(pep)]\n",
    "            neg_peps = [p for p in candidates if p != pep]\n",
    "            \n",
    "            if len(neg_peps) >= k_neg:\n",
    "                selected_negs = random.sample(neg_peps, k_neg)\n",
    "            else:\n",
    "                selected_negs = neg_peps + [self._mutate_peptide(pep) for _ in range(k_neg - len(neg_peps))]\n",
    "            \n",
    "            for neg_pep in selected_negs:\n",
    "                negatives.append({\"CDR3\": cdr3, \"MHC\": mhc, \"Epitope\": neg_pep, \"label\": 0})\n",
    "            \n",
    "            # Strategy 2: Same peptide, different TCR\n",
    "            other_tcrs = df[df[\"Epitope\"] != pep][\"CDR3\"].unique()\n",
    "            if len(other_tcrs) > 0:\n",
    "                neg_tcr = random.choice(other_tcrs)\n",
    "                negatives.append({\"CDR3\": neg_tcr, \"MHC\": mhc, \"Epitope\": pep, \"label\": 0})\n",
    "        \n",
    "        df_neg = pd.DataFrame(negatives)\n",
    "        return pd.concat([df, df_neg], ignore_index=True).drop_duplicates()\n",
    "    \n",
    "    def _mutate_peptide(self, peptide: str, n_mut: int = 1) -> str:\n",
    "        \"\"\"Mutate peptide for negative sampling\"\"\"\n",
    "        s = list(peptide)\n",
    "        for _ in range(n_mut):\n",
    "            pos = random.randint(0, len(s) - 1)\n",
    "            original = s[pos]\n",
    "            s[pos] = random.choice([aa for aa in AA_STANDARD if aa != original])\n",
    "        return \"\".join(s)\n",
    "    \n",
    "    def _split_data(self, df: pd.DataFrame):\n",
    "        \"\"\"Split data by epitope to avoid leakage\"\"\"\n",
    "        positive_epitopes = df[df[\"label\"] == 1][\"Epitope\"].unique()\n",
    "        random.shuffle(positive_epitopes)\n",
    "        \n",
    "        n_test = max(1, int(len(positive_epitopes) * self.config.test_size))\n",
    "        n_val = max(1, int(len(positive_epitopes) * self.config.val_size))\n",
    "        \n",
    "        test_epitopes = set(positive_epitopes[:n_test])\n",
    "        val_epitopes = set(positive_epitopes[n_test:n_test + n_val])\n",
    "        train_epitopes = set(positive_epitopes[n_test + n_val:])\n",
    "        \n",
    "        self.df_test = df[df[\"Epitope\"].isin(test_epitopes)].reset_index(drop=True)\n",
    "        self.df_val = df[df[\"Epitope\"].isin(val_epitopes)].reset_index(drop=True)\n",
    "        self.df_train = df[df[\"Epitope\"].isin(train_epitopes)].reset_index(drop=True)\n",
    "\n",
    "# --------------------------\n",
    "# Base Classes\n",
    "# --------------------------\n",
    "\n",
    "class BaseModel:\n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def fit(self, df_train: pd.DataFrame, df_val: pd.DataFrame):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, df_test: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Return predictions and probabilities\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class TemperatureScaling:\n",
    "    \"\"\"Temperature scaling for calibration\"\"\"\n",
    "    def __init__(self):\n",
    "        self.temperature = 1.0\n",
    "    \n",
    "    def fit(self, logits: np.ndarray, labels: np.ndarray):\n",
    "        \"\"\"Fit temperature parameter\"\"\"\n",
    "        try:\n",
    "            from scipy.optimize import minimize_scalar\n",
    "            \n",
    "            def nll(temp):\n",
    "                if temp <= 0:\n",
    "                    return 1e6\n",
    "                scaled_logits = logits / temp\n",
    "                probs = 1 / (1 + np.exp(-scaled_logits))\n",
    "                probs = np.clip(probs, 1e-7, 1 - 1e-7)\n",
    "                return -np.mean(labels * np.log(probs) + (1 - labels) * np.log(1 - probs))\n",
    "            \n",
    "            result = minimize_scalar(nll, bounds=(0.1, 10.0), method='bounded')\n",
    "            self.temperature = result.x\n",
    "        except:\n",
    "            self.temperature = 1.0\n",
    "    \n",
    "    def apply(self, logits: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply temperature scaling\"\"\"\n",
    "        return 1 / (1 + np.exp(-logits / self.temperature))\n",
    "\n",
    "class Evaluator:\n",
    "    @staticmethod\n",
    "    def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray, y_prob: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Compute evaluation metrics\"\"\"\n",
    "        return {\n",
    "            \"auc\": roc_auc_score(y_true, y_prob),\n",
    "            \"auprc\": average_precision_score(y_true, y_prob),\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": ((y_pred == 1) & (y_true == 1)).sum() / max(1, (y_pred == 1).sum()),\n",
    "            \"recall\": ((y_pred == 1) & (y_true == 1)).sum() / max(1, (y_true == 1).sum()),\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def aggregate_metrics(metrics_list: List[Dict[str, float]]) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Aggregate metrics from multiple runs with CI\"\"\"\n",
    "        aggregated = {}\n",
    "        for metric in metrics_list[0].keys():\n",
    "            values = [m[metric] for m in metrics_list]\n",
    "            mean_val = np.mean(values)\n",
    "            std_val = np.std(values)\n",
    "            ci_lower = mean_val - 1.96 * std_val / np.sqrt(len(values))\n",
    "            ci_upper = mean_val + 1.96 * std_val / np.sqrt(len(values))\n",
    "            \n",
    "            aggregated[metric] = {\n",
    "                \"mean\": mean_val,\n",
    "                \"std\": std_val,\n",
    "                \"ci_lower\": ci_lower,\n",
    "                \"ci_upper\": ci_upper\n",
    "            }\n",
    "        return aggregated\n",
    "\n",
    "# --------------------------\n",
    "# B1: ESM Fine-tuning + InfoNCE + Temperature Scaling (Main Model)\n",
    "# --------------------------\n",
    "\n",
    "class ESMSequenceEncoder:\n",
    "    def __init__(self, model_name: str, device: str, freeze_layers: int = 6):\n",
    "        print(f\"[Info] Loading ESM model: {model_name}\")\n",
    "        self.device = device\n",
    "        self.tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "        self.model = EsmModel.from_pretrained(model_name).to(device)\n",
    "        \n",
    "        # Selective unfreezing for fine-tuning\n",
    "        total_layers = len(self.model.encoder.layer)\n",
    "        for i, layer in enumerate(self.model.encoder.layer):\n",
    "            if i < freeze_layers:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            else:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "        \n",
    "        # Always fine-tune embeddings and pooler\n",
    "        for param in self.model.embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "        if hasattr(self.model, 'pooler') and self.model.pooler:\n",
    "            for param in self.model.pooler.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        self.hidden_size = self.model.config.hidden_size\n",
    "        print(f\"[Info] ESM loaded, hidden_size: {self.hidden_size}, trainable layers: {total_layers - freeze_layers}\")\n",
    "        \n",
    "    def encode_batch(self, sequences: List[str], max_length: int = 512) -> torch.Tensor:\n",
    "        clean_seqs = []\n",
    "        for seq in sequences:\n",
    "            clean_seq = \"\".join([c for c in seq.upper() if c in AA_SET])\n",
    "            clean_seqs.append(clean_seq if clean_seq else \"A\")\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            clean_seqs, return_tensors=\"pt\", padding=True, \n",
    "            truncation=True, max_length=max_length\n",
    "        ).to(self.device)\n",
    "        \n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "\n",
    "class ESMFineTuneModel(nn.Module):\n",
    "    def __init__(self, esm_encoder: ESMSequenceEncoder, d_model: int = 128, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.esm_encoder = esm_encoder\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Projectors\n",
    "        self.proj_tcr = nn.Sequential(\n",
    "            nn.Linear(esm_encoder.hidden_size, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.proj_mhc = nn.Sequential(\n",
    "            nn.Linear(esm_encoder.hidden_size, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.proj_peptide = nn.Sequential(\n",
    "            nn.Linear(esm_encoder.hidden_size, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model * 3, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "        \n",
    "        # InfoNCE projector\n",
    "        self.infonce_proj = nn.Linear(d_model * 3, d_model)\n",
    "        \n",
    "    def forward(self, cdr3_seqs: List[str], mhc_alleles: List[str], epitope_seqs: List[str]):\n",
    "        # Convert MHC alleles to sequences\n",
    "        mhc_seqs = [get_mhc_sequence(allele) for allele in mhc_alleles]\n",
    "        \n",
    "        # Encode\n",
    "        tcr_emb = self.esm_encoder.encode_batch(cdr3_seqs)\n",
    "        mhc_emb = self.esm_encoder.encode_batch(mhc_seqs)\n",
    "        pep_emb = self.esm_encoder.encode_batch(epitope_seqs)\n",
    "        \n",
    "        # Project\n",
    "        tcr_proj = self.proj_tcr(tcr_emb)\n",
    "        mhc_proj = self.proj_mhc(mhc_emb)\n",
    "        pep_proj = self.proj_peptide(pep_emb)\n",
    "        \n",
    "        # Concatenate\n",
    "        combined = torch.cat([tcr_proj, mhc_proj, pep_proj], dim=-1)\n",
    "        \n",
    "        # Classification logits\n",
    "        logits = self.classifier(combined).squeeze(-1)\n",
    "        \n",
    "        # InfoNCE features\n",
    "        infonce_features = self.infonce_proj(combined)\n",
    "        \n",
    "        return logits, infonce_features\n",
    "\n",
    "class InfoNCELoss(nn.Module):\n",
    "    def __init__(self, temperature: float = 0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, features: torch.Tensor, labels: torch.Tensor):\n",
    "        # Normalize features\n",
    "        features = F.normalize(features, dim=1)\n",
    "        \n",
    "        # Check if we have positive pairs\n",
    "        pos_mask = (labels == 1).float()\n",
    "        \n",
    "        if pos_mask.sum() < 2:\n",
    "            return torch.tensor(0.0, device=features.device)\n",
    "        \n",
    "        # Compute similarities\n",
    "        sim_matrix = torch.matmul(features, features.t()) / self.temperature\n",
    "        \n",
    "        # InfoNCE loss for positive pairs\n",
    "        pos_pairs = []\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i] == 1:\n",
    "                for j in range(i + 1, len(labels)):\n",
    "                    if labels[j] == 1:\n",
    "                        pos_pairs.append((i, j))\n",
    "        \n",
    "        if not pos_pairs:\n",
    "            return torch.tensor(0.0, device=features.device)\n",
    "        \n",
    "        loss = 0\n",
    "        for i, j in pos_pairs:\n",
    "            numerator = torch.exp(sim_matrix[i, j])\n",
    "            denominator = torch.sum(torch.exp(sim_matrix[i, :]))\n",
    "            loss += -torch.log(numerator / (denominator + 1e-8))\n",
    "        \n",
    "        return loss / len(pos_pairs)\n",
    "\n",
    "class TCRDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return {\n",
    "            \"cdr3\": str(row[\"CDR3\"]),\n",
    "            \"mhc\": str(row[\"MHC\"]),\n",
    "            \"epitope\": str(row[\"Epitope\"]),\n",
    "            \"label\": float(row[\"label\"])\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"cdr3\": [item[\"cdr3\"] for item in batch],\n",
    "        \"mhc\": [item[\"mhc\"] for item in batch],\n",
    "        \"epitope\": [item[\"epitope\"] for item in batch],\n",
    "        \"labels\": torch.tensor([item[\"label\"] for item in batch], dtype=torch.float32)\n",
    "    }\n",
    "\n",
    "class ESMFineTuneWithInfoNCE(BaseModel):\n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        super().__init__(config)\n",
    "        self.model = None\n",
    "        self.temp_scaling = TemperatureScaling()\n",
    "        \n",
    "    def fit(self, df_train: pd.DataFrame, df_val: pd.DataFrame):\n",
    "        # Initialize model\n",
    "        esm_encoder = ESMSequenceEncoder(\n",
    "            self.config.esm_model_name, \n",
    "            self.config.device,\n",
    "            self.config.esm_freeze_layers\n",
    "        )\n",
    "        \n",
    "        self.model = ESMFineTuneModel(esm_encoder, self.config.d_model).to(self.config.device)\n",
    "        \n",
    "        # Training setup\n",
    "        esm_params = []\n",
    "        proj_params = []\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'esm_encoder' in name:\n",
    "                esm_params.append(param)\n",
    "            else:\n",
    "                proj_params.append(param)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': esm_params, 'lr': self.config.lr * 0.1},\n",
    "            {'params': proj_params, 'lr': self.config.lr}\n",
    "        ], weight_decay=0.01)\n",
    "        \n",
    "        bce_loss = nn.BCEWithLogitsLoss()\n",
    "        infonce_loss = InfoNCELoss()\n",
    "        \n",
    "        # Data loaders\n",
    "        train_dataset = TCRDataset(df_train)\n",
    "        val_dataset = TCRDataset(df_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.config.batch_size, \n",
    "                                shuffle=True, collate_fn=collate_fn)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=self.config.batch_size, \n",
    "                              shuffle=False, collate_fn=collate_fn)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(self.config.epochs):\n",
    "            self.model.train()\n",
    "            for batch in train_loader:\n",
    "                labels = batch[\"labels\"].to(self.config.device)\n",
    "                \n",
    "                logits, features = self.model(batch[\"cdr3\"], batch[\"mhc\"], batch[\"epitope\"])\n",
    "                \n",
    "                # Combined loss\n",
    "                bce = bce_loss(logits, labels)\n",
    "                info_nce = infonce_loss(features, labels)\n",
    "                loss = bce + 0.1 * info_nce\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Temperature scaling\n",
    "        self.model.eval()\n",
    "        val_logits = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                labels = batch[\"labels\"].to(self.config.device)\n",
    "                logits, _ = self.model(batch[\"cdr3\"], batch[\"mhc\"], batch[\"epitope\"])\n",
    "                \n",
    "                val_logits.extend(logits.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        self.temp_scaling.fit(np.array(val_logits), np.array(val_labels))\n",
    "        \n",
    "    def predict(self, df_test: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        test_dataset = TCRDataset(df_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.config.batch_size, \n",
    "                                shuffle=False, collate_fn=collate_fn)\n",
    "        \n",
    "        all_logits = []\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                logits, _ = self.model(batch[\"cdr3\"], batch[\"mhc\"], batch[\"epitope\"])\n",
    "                all_logits.extend(logits.cpu().numpy())\n",
    "        \n",
    "        logits = np.array(all_logits)\n",
    "        probs = self.temp_scaling.apply(logits)\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "        \n",
    "        return preds, probs\n",
    "\n",
    "# --------------------------\n",
    "# Other Main Models (B2-B8) - Simplified versions\n",
    "# --------------------------\n",
    "\n",
    "class SimpleFeatureModel(BaseModel):\n",
    "    \"\"\"Simple feature-based model for B2, B7, B8\"\"\"\n",
    "    def __init__(self, config: ExperimentConfig, model_type: str = \"lr\"):\n",
    "        super().__init__(config)\n",
    "        self.model_type = model_type\n",
    "        self.classifier = None\n",
    "        self.temp_scaling = TemperatureScaling()\n",
    "        \n",
    "    def fit(self, df_train: pd.DataFrame, df_val: pd.DataFrame):\n",
    "        train_features = self._extract_features(df_train)\n",
    "        val_features = self._extract_features(df_val)\n",
    "        \n",
    "        train_labels = df_train[\"label\"].values\n",
    "        val_labels = df_val[\"label\"].values\n",
    "        \n",
    "        if self.model_type == \"lr\":\n",
    "            self.classifier = LogisticRegression(max_iter=1000, random_state=self.config.seed)\n",
    "        elif self.model_type == \"rf\":\n",
    "            self.classifier = RandomForestClassifier(n_estimators=100, random_state=self.config.seed)\n",
    "        elif self.model_type == \"svm\":\n",
    "            self.classifier = SVC(probability=True, random_state=self.config.seed)\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            self.classifier.fit(train_features, train_labels)\n",
    "        \n",
    "        # Temperature scaling\n",
    "        if hasattr(self.classifier, 'decision_function'):\n",
    "            val_logits = self.classifier.decision_function(val_features)\n",
    "        else:\n",
    "            val_probs = self.classifier.predict_proba(val_features)[:, 1]\n",
    "            val_logits = np.log(val_probs / (1 - val_probs + 1e-8))\n",
    "        \n",
    "        self.temp_scaling.fit(val_logits, val_labels)\n",
    "        \n",
    "    def _extract_features(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Extract simple sequence features\"\"\"\n",
    "        features = []\n",
    "        for _, row in df.iterrows():\n",
    "            cdr3_len = len(row[\"CDR3\"])\n",
    "            epitope_len = len(row[\"Epitope\"])\n",
    "            mhc_type = hash(row[\"MHC\"]) % 10  # Simple MHC encoding\n",
    "            \n",
    "            # Amino acid composition\n",
    "            cdr3_aa_counts = [row[\"CDR3\"].count(aa) / len(row[\"CDR3\"]) for aa in \"ACDEFGHIKLMNPQRSTVWY\"]\n",
    "            epitope_aa_counts = [row[\"Epitope\"].count(aa) / len(row[\"Epitope\"]) for aa in \"ACDEFGHIKLMNPQRSTVWY\"]\n",
    "            \n",
    "            # Simple physicochemical properties\n",
    "            hydrophobic_aas = set(\"AILMFPWV\")\n",
    "            charged_aas = set(\"DEKR\")\n",
    "            \n",
    "            cdr3_hydrophobic = sum(1 for aa in row[\"CDR3\"] if aa in hydrophobic_aas) / len(row[\"CDR3\"])\n",
    "            cdr3_charged = sum(1 for aa in row[\"CDR3\"] if aa in charged_aas) / len(row[\"CDR3\"])\n",
    "            epitope_hydrophobic = sum(1 for aa in row[\"Epitope\"] if aa in hydrophobic_aas) / len(row[\"Epitope\"])\n",
    "            epitope_charged = sum(1 for aa in row[\"Epitope\"] if aa in charged_aas) / len(row[\"Epitope\"])\n",
    "            \n",
    "            feature_vector = ([cdr3_len, epitope_len, mhc_type, cdr3_hydrophobic, cdr3_charged, \n",
    "                             epitope_hydrophobic, epitope_charged] + cdr3_aa_counts + epitope_aa_counts)\n",
    "            features.append(feature_vector)\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def predict(self, df_test: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        test_features = self._extract_features(df_test)\n",
    "        \n",
    "        if hasattr(self.classifier, 'decision_function'):\n",
    "            logits = self.classifier.decision_function(test_features)\n",
    "        else:\n",
    "            probs = self.classifier.predict_proba(test_features)[:, 1]\n",
    "            logits = np.log(probs / (1 - probs + 1e-8))\n",
    "        \n",
    "        probs = self.temp_scaling.apply(logits)\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "        \n",
    "        return preds, probs\n",
    "\n",
    "# --------------------------\n",
    "# BX1: ProtBert-based Model (Auxiliary)\n",
    "# --------------------------\n",
    "\n",
    "class ProtBertModel(BaseModel):\n",
    "    \"\"\"ProtBert-based model as auxiliary method\"\"\"\n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        super().__init__(config)\n",
    "        self.model = None\n",
    "        self.temp_scaling = TemperatureScaling()\n",
    "        \n",
    "    def fit(self, df_train: pd.DataFrame, df_val: pd.DataFrame):\n",
    "        # For demo, use simplified ProtBert-like features\n",
    "        # In practice, you would load actual ProtBert model\n",
    "        train_features = self._extract_protbert_features(df_train)\n",
    "        val_features = self._extract_protbert_features(df_val)\n",
    "        \n",
    "        train_labels = df_train[\"label\"].values\n",
    "        val_labels = df_val[\"label\"].values\n",
    "        \n",
    "        self.classifier = LogisticRegression(max_iter=1000, random_state=self.config.seed)\n",
    "        self.classifier.fit(train_features, train_labels)\n",
    "        \n",
    "        val_logits = self.classifier.decision_function(val_features)\n",
    "        self.temp_scaling.fit(val_logits, val_labels)\n",
    "        \n",
    "    def _extract_protbert_features(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Extract ProtBert-like features (simplified)\"\"\"\n",
    "        features = []\n",
    "        for _, row in df.iterrows():\n",
    "            # Simulate ProtBert embeddings with enhanced features\n",
    "            cdr3_seq = row[\"CDR3\"]\n",
    "            epitope_seq = row[\"Epitope\"]\n",
    "            mhc_seq = get_mhc_sequence(row[\"MHC\"])\n",
    "            \n",
    "            # More sophisticated features than simple model\n",
    "            feature_vector = []\n",
    "            \n",
    "            # Length features\n",
    "            feature_vector.extend([len(cdr3_seq), len(epitope_seq), len(mhc_seq)])\n",
    "            \n",
    "            # N-gram features (simulate BERT-like attention to subsequences)\n",
    "            for seq in [cdr3_seq, epitope_seq, mhc_seq[:20]]:  # Truncate MHC\n",
    "                for k in [2, 3]:\n",
    "                    ngrams = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "                    ngram_hash = sum(hash(ngram) % 1000 for ngram in ngrams) / 1000  # Normalize\n",
    "                    feature_vector.append(ngram_hash)\n",
    "            \n",
    "            # Positional encoding simulation\n",
    "            for i, aa in enumerate(cdr3_seq[:20]):  # Truncate\n",
    "                pos_encoding = math.sin(i / 10000) if aa in \"ACDEFGHIKLMNPQRSTVWY\" else 0\n",
    "                feature_vector.append(pos_encoding)\n",
    "            \n",
    "            # Pad to fixed length\n",
    "            while len(feature_vector) < 100:\n",
    "                feature_vector.append(0.0)\n",
    "            \n",
    "            features.append(feature_vector[:100])\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def predict(self, df_test: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        test_features = self._extract_protbert_features(df_test)\n",
    "        logits = self.classifier.decision_function(test_features)\n",
    "        probs = self.temp_scaling.apply(logits)\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "        return preds, probs\n",
    "\n",
    "# --------------------------\n",
    "# BX2: Simple Graph Neural Network (Auxiliary)\n",
    "# --------------------------\n",
    "\n",
    "class SimpleGraphModel(BaseModel):\n",
    "    \"\"\"Simple graph-based model using sequence similarity\"\"\"\n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        super().__init__(config)\n",
    "        self.train_data = None\n",
    "        self.similarity_threshold = 0.7\n",
    "        \n",
    "    def fit(self, df_train: pd.DataFrame, df_val: pd.DataFrame):\n",
    "        self.train_data = df_train.copy()\n",
    "        \n",
    "    def predict(self, df_test: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "        \n",
    "        for _, test_row in df_test.iterrows():\n",
    "            test_tcr = test_row[\"CDR3\"]\n",
    "            test_epitope = test_row[\"Epitope\"]\n",
    "            \n",
    "            # Find similar examples in training data\n",
    "            similar_scores = []\n",
    "            \n",
    "            for _, train_row in self.train_data.iterrows():\n",
    "                tcr_sim = compute_sequence_similarity(test_tcr, train_row[\"CDR3\"])\n",
    "                epitope_sim = compute_sequence_similarity(test_epitope, train_row[\"Epitope\"])\n",
    "                \n",
    "                # Graph-like aggregation: require both TCR and epitope similarity\n",
    "                combined_sim = (tcr_sim + epitope_sim) / 2\n",
    "                \n",
    "                if combined_sim > self.similarity_threshold:\n",
    "                    similar_scores.append(train_row[\"label\"])\n",
    "            \n",
    "            if similar_scores:\n",
    "                prob = np.mean(similar_scores)\n",
    "            else:\n",
    "                prob = 0.5  # Default for no similar examples\n",
    "            \n",
    "            pred = 1 if prob > 0.5 else 0\n",
    "            predictions.append(pred)\n",
    "            probabilities.append(prob)\n",
    "        \n",
    "        return np.array(predictions), np.array(probabilities)\n",
    "\n",
    "# --------------------------\n",
    "# BX3: Simple VAE-like Model (Auxiliary)\n",
    "# --------------------------\n",
    "\n",
    "class SimpleVAEModel(BaseModel):\n",
    "    \"\"\"Simple VAE-inspired model for sequence embedding\"\"\"\n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        super().__init__(config)\n",
    "        self.encoder_tcr = None\n",
    "        self.encoder_epitope = None\n",
    "        self.classifier = None\n",
    "        self.temp_scaling = TemperatureScaling()\n",
    "        \n",
    "    def fit(self, df_train: pd.DataFrame, df_val: pd.DataFrame):\n",
    "        # Extract latent representations\n",
    "        train_features = self._extract_vae_features(df_train)\n",
    "        val_features = self._extract_vae_features(df_val)\n",
    "        \n",
    "        train_labels = df_train[\"label\"].values\n",
    "        val_labels = df_val[\"label\"].values\n",
    "        \n",
    "        self.classifier = LogisticRegression(max_iter=1000, random_state=self.config.seed)\n",
    "        self.classifier.fit(train_features, train_labels)\n",
    "        \n",
    "        val_logits = self.classifier.decision_function(val_features)\n",
    "        self.temp_scaling.fit(val_logits, val_labels)\n",
    "        \n",
    "    def _extract_vae_features(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Extract VAE-like latent features\"\"\"\n",
    "        features = []\n",
    "        for _, row in df.iterrows():\n",
    "            # Simulate VAE latent space with statistical moments\n",
    "            cdr3_seq = row[\"CDR3\"]\n",
    "            epitope_seq = row[\"Epitope\"]\n",
    "            \n",
    "            # Mean and variance-like features (simulate mu and sigma from VAE)\n",
    "            cdr3_ascii = [ord(aa) for aa in cdr3_seq]\n",
    "            epitope_ascii = [ord(aa) for aa in epitope_seq]\n",
    "            \n",
    "            # Statistical moments as latent features\n",
    "            feature_vector = [\n",
    "                np.mean(cdr3_ascii), np.std(cdr3_ascii),\n",
    "                np.mean(epitope_ascii), np.std(epitope_ascii),\n",
    "                np.median(cdr3_ascii), np.median(epitope_ascii),\n",
    "                np.max(cdr3_ascii) - np.min(cdr3_ascii),  # Range\n",
    "                np.max(epitope_ascii) - np.min(epitope_ascii),\n",
    "                len(cdr3_seq), len(epitope_seq)\n",
    "            ]\n",
    "            \n",
    "            # Add \"learned\" latent dimensions (random projections as simulation)\n",
    "            for i in range(self.config.vae_latent_dim - 10):\n",
    "                projection = sum(hash(f\"{cdr3_seq}_{epitope_seq}_{i}\") % 100 for _ in range(1)) / 100\n",
    "                feature_vector.append(projection)\n",
    "            \n",
    "            features.append(feature_vector[:self.config.vae_latent_dim])\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def predict(self, df_test: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        test_features = self._extract_vae_features(df_test)\n",
    "        logits = self.classifier.decision_function(test_features)\n",
    "        probs = self.temp_scaling.apply(logits)\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "        return preds, probs\n",
    "\n",
    "# --------------------------\n",
    "# Experiment Runner\n",
    "# --------------------------\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        self.config = config\n",
    "        self.data_manager = DataManager(config)\n",
    "        self.evaluator = Evaluator()\n",
    "        \n",
    "    def run_all_experiments(self) -> pd.DataFrame:\n",
    "        \"\"\"Run all experiments and return results table\"\"\"\n",
    "        print(\"Loading and preparing data...\")\n",
    "        self.data_manager.load_and_split_data()\n",
    "        \n",
    "        # Complete model set: Main Table + Auxiliary Methods\n",
    "        models = {\n",
    "            # Main Models\n",
    "            \"B1_ESM_InfoNCE_TempScale\": ESMFineTuneWithInfoNCE,  # Our main model\n",
    "            \"B2_Simple_Features_LR\": lambda config: SimpleFeatureModel(config, \"lr\"),\n",
    "            \"B3_Simple_Features_RF\": lambda config: SimpleFeatureModel(config, \"rf\"),\n",
    "            \"B4_Simple_Features_SVM\": lambda config: SimpleFeatureModel(config, \"svm\"),\n",
    "            \n",
    "            # Auxiliary Models (BX series)\n",
    "            \"BX1_ProtBert_Like\": ProtBertModel,\n",
    "            \"BX2_Simple_Graph\": SimpleGraphModel,\n",
    "            \"BX3_VAE_Like\": SimpleVAEModel,\n",
    "        }\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for model_name, model_class in models.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Running {model_name}...\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            model_results = []\n",
    "            \n",
    "            for run in range(self.config.n_runs):\n",
    "                print(f\"Run {run + 1}/{self.config.n_runs}\")\n",
    "                \n",
    "                # Set seed for reproducibility\n",
    "                set_seed(self.config.seed + run)\n",
    "                \n",
    "                try:\n",
    "                    # Initialize and train model\n",
    "                    model = model_class(self.config)\n",
    "                    model.fit(self.data_manager.df_train, self.data_manager.df_val)\n",
    "                    \n",
    "                    # Predict on test set\n",
    "                    y_pred, y_prob = model.predict(self.data_manager.df_test)\n",
    "                    y_true = self.data_manager.df_test[\"label\"].values\n",
    "                    \n",
    "                    # Compute metrics\n",
    "                    metrics = self.evaluator.compute_metrics(y_true, y_pred, y_prob)\n",
    "                    model_results.append(metrics)\n",
    "                    \n",
    "                    print(f\"  AUC: {metrics['auc']:.4f}, AUPRC: {metrics['auprc']:.4f}, Acc: {metrics['accuracy']:.4f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error in {model_name} run {run + 1}: {str(e)}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "            \n",
    "            if model_results:\n",
    "                # Aggregate results\n",
    "                aggregated = self.evaluator.aggregate_metrics(model_results)\n",
    "                \n",
    "                for metric, stats in aggregated.items():\n",
    "                    results.append({\n",
    "                        \"Model\": model_name,\n",
    "                        \"Metric\": metric.upper(),\n",
    "                        \"Mean\": stats[\"mean\"],\n",
    "                        \"Std\": stats[\"std\"],\n",
    "                        \"CI_Lower\": stats[\"ci_lower\"],\n",
    "                        \"CI_Upper\": stats[\"ci_upper\"],\n",
    "                        \"CI_String\": f\"{stats['mean']:.4f} ± {1.96 * stats['std'] / np.sqrt(self.config.n_runs):.4f}\"\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"  No successful runs for {model_name}\")\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def save_results(self, results_df: pd.DataFrame, output_path: str = \"comprehensive_results.csv\"):\n",
    "        \"\"\"Save results to CSV and print summary table\"\"\"\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        # Create summary table\n",
    "        print(f\"\\n{'='*120}\")\n",
    "        print(\"COMPREHENSIVE TCR-PEPTIDE-MHC BINDING PREDICTION BENCHMARK\")\n",
    "        print(f\"{'='*120}\")\n",
    "        \n",
    "        # Pivot table for better readability\n",
    "        pivot_df = results_df.pivot(index=\"Model\", columns=\"Metric\", values=\"CI_String\")\n",
    "        \n",
    "        # Reorder columns for better presentation\n",
    "        metric_order = [\"AUC\", \"AUPRC\", \"ACCURACY\", \"PRECISION\", \"RECALL\"]\n",
    "        available_metrics = [col for col in metric_order if col in pivot_df.columns]\n",
    "        pivot_df = pivot_df[available_metrics]\n",
    "        \n",
    "        print(pivot_df.to_string())\n",
    "        \n",
    "        # Highlight best performing models\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"PERFORMANCE HIGHLIGHTS:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        auc_results = results_df[results_df[\"Metric\"] == \"AUC\"].sort_values(\"Mean\", ascending=False)\n",
    "        print(\"Top 3 Models by AUC:\")\n",
    "        for i, (_, row) in enumerate(auc_results.head(3).iterrows()):\n",
    "            print(f\"{i+1}. {row['Model']}: {row['CI_String']}\")\n",
    "        \n",
    "        print(f\"\\nDetailed results saved to: {output_path}\")\n",
    "        print(f\"{'='*120}\")\n",
    "\n",
    "# --------------------------\n",
    "# Main Function\n",
    "# --------------------------\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main experiment runner\"\"\"\n",
    "    # Configuration\n",
    "    config = ExperimentConfig(\n",
    "        seed=42,\n",
    "        n_runs=3,\n",
    "        epochs=8,  # Increased for main model\n",
    "        batch_size=32,\n",
    "        lr=2e-4,\n",
    "        d_model=128\n",
    "    )\n",
    "    \n",
    "    print(\"Starting comprehensive TCR-peptide-MHC binding prediction benchmark...\")\n",
    "    print(f\"Device: {config.device}\")\n",
    "    print(f\"Configuration: epochs={config.epochs}, batch_size={config.batch_size}, lr={config.lr}\")\n",
    "    \n",
    "    # Run experiments\n",
    "    runner = ExperimentRunner(config)\n",
    "    results_df = runner.run_all_experiments()\n",
    "    \n",
    "    # Save and display results\n",
    "    runner.save_results(results_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04044ce-a154-47e8-9320-04c58ba3048d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
