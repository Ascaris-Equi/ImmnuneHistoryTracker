{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7148144-c8f2-4f80-9f59-1c33af5be514",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 18:13:35.957687: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-23 18:13:37.137574: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Enhanced TCR Analysis Pipeline with Logging and Cross-Validation\n",
      "======================================================================\n",
      "[Info] Loading training data...\n",
      "[Info] Loaded 5004 training examples\n",
      "[Step] Generating enhanced negative samples with positive balancing...\n",
      "[Info] Generated 209419 negative samples for 5004 positive samples\n",
      "[Info] Balancing dataset: target positive samples = 174515\n",
      "[Info] Augmenting 169511 positive samples...\n",
      "[Info] Augmented positive samples: 5004 -> 166094\n",
      "[Info] Final balanced dataset: 166094 positives + 86644 negatives = 252738 total\n",
      "[Info] Positive/Negative ratio: 1.92\n",
      "[Step] Running 5-fold cross-validation...\n",
      "\n",
      "======================================================================\n",
      "RUNNING 5-FOLD CROSS-VALIDATION\n",
      "======================================================================\n",
      "\n",
      "[Fold 1/5] Starting training...\n",
      "[Fold 1] Train: 175813 samples, Val: 54811 samples\n",
      "[Info] Loading Enhanced ESM model: facebook/esm2_t12_35M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Enhanced ESM loaded - Hidden size: 480, Trainable layers: 10/12\n",
      "[Info] Training Enhanced Model with Logging - Device: cuda\n",
      "[Info] Mixed Precision: bf16\n",
      "[Info] Epochs: 10, Batch Size: 64, Model Dim: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 2747/2747 [18:36<00:00,  2.46it/s, loss=0.6077, bce=0.5257, infonce=0.5860]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Train: Loss=0.6652, BCE=0.5791, InfoNCE=0.6150\n",
      "  Val: Loss=0.6084, AUC=0.5720, AUPRC=0.7591\n",
      "  Val: Acc=0.7261, F1=0.8357, Prec=0.7314, Rec=0.9748\n",
      "  LR=6.00e-07, Temp=0.0783, Time=1116.9s+184.9s\n",
      "  ✓ New best model! AUC: 0.5720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 2747/2747 [18:08<00:00,  2.52it/s, loss=0.5458, bce=0.4547, infonce=0.5062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "  Train: Loss=0.6419, BCE=0.5367, InfoNCE=0.5846\n",
      "  Val: Loss=0.6646, AUC=0.5092, AUPRC=0.7104\n",
      "  Val: Acc=0.7216, F1=0.8318, Prec=0.7321, Rec=0.9631\n",
      "  LR=6.09e-07, Temp=0.0814, Time=1088.9s+181.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 2747/2747 [18:04<00:00,  2.53it/s, loss=0.6427, bce=0.5324, infonce=0.5014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "  Train: Loss=0.6381, BCE=0.5139, InfoNCE=0.5644\n",
      "  Val: Loss=0.7143, AUC=0.5190, AUPRC=0.7228\n",
      "  Val: Acc=0.6867, F1=0.7994, Prec=0.7370, Rec=0.8733\n",
      "  LR=3.71e-06, Temp=0.0837, Time=1084.7s+181.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 2747/2747 [18:04<00:00,  2.53it/s, loss=0.6059, bce=0.4664, infonce=0.5363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "  Train: Loss=0.6067, BCE=0.4702, InfoNCE=0.5251\n",
      "  Val: Loss=0.7235, AUC=0.5440, AUPRC=0.7495\n",
      "  Val: Acc=0.7160, F1=0.8287, Prec=0.7284, Rec=0.9610\n",
      "  LR=6.14e-07, Temp=0.0844, Time=1084.2s+181.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 2747/2747 [18:03<00:00,  2.54it/s, loss=0.6621, bce=0.4943, infonce=0.5592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "  Train: Loss=0.6127, BCE=0.4585, InfoNCE=0.5141\n",
      "  Val: Loss=0.7257, AUC=0.5794, AUPRC=0.8000\n",
      "  Val: Acc=0.7142, F1=0.8279, Prec=0.7268, Rec=0.9618\n",
      "  LR=4.83e-06, Temp=0.0842, Time=1083.5s+181.7s\n",
      "  ✓ New best model! AUC: 0.5794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training: 100%|██████████| 2747/2747 [18:05<00:00,  2.53it/s, loss=0.5675, bce=0.4049, infonce=0.4783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "  Train: Loss=0.6250, BCE=0.4520, InfoNCE=0.5090\n",
      "  Val: Loss=0.7637, AUC=0.5711, AUPRC=0.8017\n",
      "  Val: Acc=0.7056, F1=0.8213, Prec=0.7254, Rec=0.9464\n",
      "  LR=3.71e-06, Temp=0.0845, Time=1085.1s+182.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training: 100%|██████████| 2747/2747 [18:04<00:00,  2.53it/s, loss=0.5542, bce=0.3869, infonce=0.4403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "  Train: Loss=0.5931, BCE=0.4135, InfoNCE=0.4725\n",
      "  Val: Loss=0.7592, AUC=0.6198, AUPRC=0.8303\n",
      "  Val: Acc=0.7109, F1=0.8270, Prec=0.7226, Rec=0.9665\n",
      "  LR=2.05e-06, Temp=0.0838, Time=1084.7s+181.5s\n",
      "  ✓ New best model! AUC: 0.6198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Training: 100%|██████████| 2747/2747 [18:05<00:00,  2.53it/s, loss=0.5268, bce=0.3516, infonce=0.4170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "  Train: Loss=0.5601, BCE=0.3762, InfoNCE=0.4379\n",
      "  Val: Loss=0.7603, AUC=0.6373, AUPRC=0.8378\n",
      "  Val: Acc=0.7154, F1=0.8301, Prec=0.7241, Rec=0.9725\n",
      "  LR=6.16e-07, Temp=0.0837, Time=1085.3s+181.8s\n",
      "  ✓ New best model! AUC: 0.6373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training: 100%|██████████| 2747/2747 [18:05<00:00,  2.53it/s, loss=0.5436, bce=0.3720, infonce=0.3730]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "  Train: Loss=0.5518, BCE=0.3584, InfoNCE=0.4203\n",
      "  Val: Loss=0.7212, AUC=0.6146, AUPRC=0.8258\n",
      "  Val: Acc=0.7103, F1=0.8245, Prec=0.7271, Rec=0.9520\n",
      "  LR=5.00e-06, Temp=0.0822, Time=1085.1s+181.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Training: 100%|██████████| 2747/2747 [18:04<00:00,  2.53it/s, loss=0.7248, bce=0.4564, infonce=0.5370]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "  Train: Loss=0.6508, BCE=0.4142, InfoNCE=0.4733\n",
      "  Val: Loss=0.7106, AUC=0.6251, AUPRC=0.8317\n",
      "  Val: Acc=0.7154, F1=0.8272, Prec=0.7307, Rec=0.9531\n",
      "  LR=4.83e-06, Temp=0.0809, Time=1084.7s+182.0s\n",
      "[Info] Training history saved to training_logs/fold_1/training_history.csv\n",
      "[Info] Training history saved to training_logs/fold_1/training_history.json\n",
      "[Info] Training curves saved to training_logs/fold_1/training_curves.png\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Accelerator.load_state() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1336\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1336\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 1315\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;66;03m# Run 5-fold cross-validation\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Step] Running 5-fold cross-validation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1315\u001b[0m cv_evaluator, cv_summary \u001b[38;5;241m=\u001b[39m \u001b[43mrun_cross_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mesm_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/esm2_t12_35M_UR50D\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ ENHANCED ANALYSIS WITH LOGGING AND CV COMPLETED!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 1209\u001b[0m, in \u001b[0;36mrun_cross_validation\u001b[0;34m(df_all, esm_model_name, d_model, lr, batch_size, epochs)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;66;03m# Train model for this fold\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m fold_log_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_logs/fold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1209\u001b[0m model, best_auc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_enhanced_model_with_logging\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mesm_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_log_dir\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# Move model to CPU for evaluation\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcpu()\n",
      "Cell \u001b[0;32mIn[1], line 1154\u001b[0m, in \u001b[0;36mtrain_enhanced_model_with_logging\u001b[0;34m(df_train, df_val, esm_encoder, d_model, lr, batch_size, epochs, log_dir)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;66;03m# Load best model\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39mis_local_main_process \u001b[38;5;129;01mand\u001b[39;00m best_model_state:\n\u001b[0;32m-> 1154\u001b[0m     \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_model_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Info] Loaded best model with AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1157\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mwait_for_everyone()\n",
      "\u001b[0;31mTypeError\u001b[0m: Accelerator.load_state() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Enhanced ESM Fine-tuning for TCR-peptide-MHC binding prediction with Maximum Accuracy\n",
    "# Dependencies: torch, transformers, numpy, pandas, scikit-learn, tqdm, requests, accelerate\n",
    "\n",
    "import os, sys, math, json, time, random, requests, warnings\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import EsmModel, EsmTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, precision_recall_curve, roc_curve\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# New imports for acceleration\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed as accelerate_set_seed\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please install tqdm: pip install tqdm\")\n",
    "\n",
    "# --------------------------\n",
    "# Helper Functions\n",
    "# --------------------------\n",
    "\n",
    "def get_model_attr(model, attr_name):\n",
    "    \"\"\"Safely get model attribute, handling accelerator wrapping\"\"\"\n",
    "    if hasattr(model, 'module'):\n",
    "        return getattr(model.module, attr_name)\n",
    "    else:\n",
    "        return getattr(model, attr_name)\n",
    "\n",
    "def set_model_attr(model, attr_name, value):\n",
    "    \"\"\"Safely set model attribute, handling accelerator wrapping\"\"\"\n",
    "    if hasattr(model, 'module'):\n",
    "        setattr(model.module, attr_name, value)\n",
    "    else:\n",
    "        setattr(model, attr_name, value)\n",
    "\n",
    "# --------------------------\n",
    "# Training History Logger\n",
    "# --------------------------\n",
    "\n",
    "class TrainingLogger:\n",
    "    \"\"\"Enhanced training logger for comprehensive metrics tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir: str = \"training_logs\"):\n",
    "        self.log_dir = log_dir\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "        self.history = {\n",
    "            'epoch': [],\n",
    "            'train_loss': [],\n",
    "            'train_bce_loss': [],\n",
    "            'train_infonce_loss': [],\n",
    "            'val_loss': [],\n",
    "            'val_bce_loss': [],\n",
    "            'val_infonce_loss': [],\n",
    "            'val_auc': [],\n",
    "            'val_auprc': [],\n",
    "            'val_accuracy': [],\n",
    "            'val_precision': [],\n",
    "            'val_recall': [],\n",
    "            'val_f1': [],\n",
    "            'learning_rate': [],\n",
    "            'temperature': [],\n",
    "            'train_time': [],\n",
    "            'val_time': []\n",
    "        }\n",
    "        \n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def log_epoch(self, epoch_data: Dict):\n",
    "        \"\"\"Log data for one epoch\"\"\"\n",
    "        for key, value in epoch_data.items():\n",
    "            if key in self.history:\n",
    "                self.history[key].append(value)\n",
    "    \n",
    "    def save_logs(self, filename: str = \"training_history.csv\"):\n",
    "        \"\"\"Save training history to CSV\"\"\"\n",
    "        df = pd.DataFrame(self.history)\n",
    "        filepath = os.path.join(self.log_dir, filename)\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"[Info] Training history saved to {filepath}\")\n",
    "        return filepath\n",
    "    \n",
    "    def save_json(self, filename: str = \"training_history.json\"):\n",
    "        \"\"\"Save training history to JSON\"\"\"\n",
    "        filepath = os.path.join(self.log_dir, filename)\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.history, f, indent=2)\n",
    "        print(f\"[Info] Training history saved to {filepath}\")\n",
    "        return filepath\n",
    "    \n",
    "    def plot_training_curves(self, filename: str = \"training_curves.png\"):\n",
    "        \"\"\"Generate training curve plots\"\"\"\n",
    "        if len(self.history['epoch']) == 0:\n",
    "            return\n",
    "            \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('Training Progress', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        epochs = self.history['epoch']\n",
    "        \n",
    "        # Loss curves\n",
    "        axes[0, 0].plot(epochs, self.history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "        axes[0, 0].plot(epochs, self.history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "        axes[0, 0].set_title('Total Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # BCE Loss\n",
    "        axes[0, 1].plot(epochs, self.history['train_bce_loss'], 'b-', label='Train BCE', linewidth=2)\n",
    "        axes[0, 1].plot(epochs, self.history['val_bce_loss'], 'r-', label='Val BCE', linewidth=2)\n",
    "        axes[0, 1].set_title('BCE Loss')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('BCE Loss')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # InfoNCE Loss\n",
    "        axes[0, 2].plot(epochs, self.history['train_infonce_loss'], 'b-', label='Train InfoNCE', linewidth=2)\n",
    "        axes[0, 2].plot(epochs, self.history['val_infonce_loss'], 'r-', label='Val InfoNCE', linewidth=2)\n",
    "        axes[0, 2].set_title('InfoNCE Loss')\n",
    "        axes[0, 2].set_xlabel('Epoch')\n",
    "        axes[0, 2].set_ylabel('InfoNCE Loss')\n",
    "        axes[0, 2].legend()\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # AUC and AUPRC\n",
    "        axes[1, 0].plot(epochs, self.history['val_auc'], 'g-', label='AUC', linewidth=2)\n",
    "        axes[1, 0].plot(epochs, self.history['val_auprc'], 'orange', label='AUPRC', linewidth=2)\n",
    "        axes[1, 0].set_title('Validation Metrics')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Score')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        axes[1, 0].set_ylim(0, 1)\n",
    "        \n",
    "        # Accuracy and F1\n",
    "        axes[1, 1].plot(epochs, self.history['val_accuracy'], 'purple', label='Accuracy', linewidth=2)\n",
    "        axes[1, 1].plot(epochs, self.history['val_f1'], 'brown', label='F1 Score', linewidth=2)\n",
    "        axes[1, 1].set_title('Classification Metrics')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Score')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].set_ylim(0, 1)\n",
    "        \n",
    "        # Learning Rate and Temperature\n",
    "        axes[1, 2].plot(epochs, self.history['learning_rate'], 'navy', label='Learning Rate', linewidth=2)\n",
    "        ax2 = axes[1, 2].twinx()\n",
    "        ax2.plot(epochs, self.history['temperature'], 'red', label='Temperature', linewidth=2)\n",
    "        axes[1, 2].set_title('Learning Rate & Temperature')\n",
    "        axes[1, 2].set_xlabel('Epoch')\n",
    "        axes[1, 2].set_ylabel('Learning Rate')\n",
    "        ax2.set_ylabel('Temperature')\n",
    "        axes[1, 2].legend(loc='upper left')\n",
    "        ax2.legend(loc='upper right')\n",
    "        axes[1, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filepath = os.path.join(self.log_dir, filename)\n",
    "        plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"[Info] Training curves saved to {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "# --------------------------\n",
    "# Cross-Validation and Metrics\n",
    "# --------------------------\n",
    "\n",
    "class CrossValidationEvaluator:\n",
    "    \"\"\"5-fold cross-validation with confidence intervals\"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits: int = 5, random_state: int = 42):\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        self.results = defaultdict(list)\n",
    "        \n",
    "    def compute_detailed_metrics(self, y_true: np.ndarray, y_prob: np.ndarray, threshold: float = 0.5) -> Dict:\n",
    "        \"\"\"Compute detailed classification metrics\"\"\"\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        \n",
    "        # Basic metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # Handle edge cases for precision/recall\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        # AUC metrics\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else 0.0\n",
    "            auprc = average_precision_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else 0.0\n",
    "        except:\n",
    "            auc = 0.0\n",
    "            auprc = 0.0\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc,\n",
    "            'auprc': auprc\n",
    "        }\n",
    "    \n",
    "    def compute_confidence_interval(self, values: List[float], confidence: float = 0.95) -> Tuple[float, float, float]:\n",
    "        \"\"\"Compute mean and confidence interval using bootstrap\"\"\"\n",
    "        values = np.array(values)\n",
    "        \n",
    "        if len(values) == 0:\n",
    "            return 0.0, 0.0, 0.0\n",
    "        \n",
    "        mean_val = np.mean(values)\n",
    "        \n",
    "        # Bootstrap confidence interval\n",
    "        n_bootstrap = 1000\n",
    "        bootstrap_means = []\n",
    "        \n",
    "        for _ in range(n_bootstrap):\n",
    "            bootstrap_sample = np.random.choice(values, size=len(values), replace=True)\n",
    "            bootstrap_means.append(np.mean(bootstrap_sample))\n",
    "        \n",
    "        bootstrap_means = np.array(bootstrap_means)\n",
    "        alpha = 1 - confidence\n",
    "        lower_percentile = (alpha / 2) * 100\n",
    "        upper_percentile = (1 - alpha / 2) * 100\n",
    "        \n",
    "        ci_lower = np.percentile(bootstrap_means, lower_percentile)\n",
    "        ci_upper = np.percentile(bootstrap_means, upper_percentile)\n",
    "        \n",
    "        return mean_val, ci_lower, ci_upper\n",
    "    \n",
    "    def add_fold_result(self, fold: int, metrics: Dict):\n",
    "        \"\"\"Add results from one fold\"\"\"\n",
    "        self.results[fold] = metrics\n",
    "        \n",
    "        # Add to overall results\n",
    "        for metric_name, value in metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                self.results[f'all_{metric_name}'].append(value)\n",
    "    \n",
    "    def get_summary_with_ci(self, confidence: float = 0.95) -> Dict:\n",
    "        \"\"\"Get summary statistics with confidence intervals\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        metric_names = ['accuracy', 'precision', 'recall', 'f1', 'auc', 'auprc']\n",
    "        \n",
    "        for metric in metric_names:\n",
    "            values = self.results.get(f'all_{metric}', [])\n",
    "            if values:\n",
    "                mean_val, ci_lower, ci_upper = self.compute_confidence_interval(values, confidence)\n",
    "                summary[metric] = {\n",
    "                    'mean': mean_val,\n",
    "                    'std': np.std(values),\n",
    "                    'ci_lower': ci_lower,\n",
    "                    'ci_upper': ci_upper,\n",
    "                    'values': values\n",
    "                }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def save_results(self, filepath: str = \"cv_results.json\"):\n",
    "        \"\"\"Save cross-validation results\"\"\"\n",
    "        # Convert defaultdict to regular dict for JSON serialization\n",
    "        results_dict = dict(self.results)\n",
    "        \n",
    "        # Add summary statistics\n",
    "        results_dict['summary'] = self.get_summary_with_ci()\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results_dict, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"[Info] Cross-validation results saved to {filepath}\")\n",
    "        return filepath\n",
    "    \n",
    "    def plot_cv_results(self, filepath: str = \"cv_results.png\"):\n",
    "        \"\"\"Plot cross-validation results\"\"\"\n",
    "        summary = self.get_summary_with_ci()\n",
    "        \n",
    "        if not summary:\n",
    "            return\n",
    "        \n",
    "        metrics = list(summary.keys())\n",
    "        means = [summary[m]['mean'] for m in metrics]\n",
    "        ci_lowers = [summary[m]['ci_lower'] for m in metrics]\n",
    "        ci_uppers = [summary[m]['ci_upper'] for m in metrics]\n",
    "        \n",
    "        # Error bars\n",
    "        errors = [\n",
    "            [means[i] - ci_lowers[i] for i in range(len(means))],\n",
    "            [ci_uppers[i] - means[i] for i in range(len(means))]\n",
    "        ]\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Bar plot with error bars\n",
    "        bars = plt.bar(metrics, means, yerr=errors, capsize=10, alpha=0.7, \n",
    "                      color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum', 'lightsalmon'])\n",
    "        \n",
    "        plt.title('5-Fold Cross-Validation Results with 95% Confidence Intervals', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('Score', fontsize=12)\n",
    "        plt.xlabel('Metrics', fontsize=12)\n",
    "        plt.ylim(0, 1.1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (bar, mean_val, ci_l, ci_u) in enumerate(zip(bars, means, ci_lowers, ci_uppers)):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                    f'{mean_val:.3f}\\n[{ci_l:.3f}, {ci_u:.3f}]',\n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"[Info] Cross-validation plot saved to {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "# --------------------------\n",
    "# Enhanced Configuration\n",
    "# --------------------------\n",
    "\n",
    "AA_STANDARD = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "AA_SET = set(AA_STANDARD)\n",
    "\n",
    "# Extended MHC Class I pseudo-sequences\n",
    "MHC_PSEUDO_SEQUENCES = {\n",
    "    \"HLA-A*02:01\": \"GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETLQRTDAPKTHMTHHAVSDHEATLRCWALSFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGQEQRYTCHVQHEGLPKPLTLRWE\",\n",
    "    \"HLA-A*01:01\": \"GSHSMRYFFTSVSRPGRGEPRFIAMGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDRETQKAKGNEQSFRVDLRTLLGYYNQSEDGSHTIQIMYGCDVGPDGRLLRGYDQYAYDGKDYIALNEDLRSWTAADTAAQITQRKWEAARVAEQLRAYLEGTCVEWLRRYLENGKDKLERADPPKTHVTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWE\",\n",
    "    \"HLA-B*07:02\": \"GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDRETQKAKGNEQSFRVDLRTLLGYYNQSEDGSHTIQIMYGCDVGPDGRLLRGYDQYAYDGKDYIALNEDLRSWTAADTAAQITQRKWEAARVAEQLRAYLEGTCVEWLRRYLENGKDKLERADPPKTHVTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWE\",\n",
    "    \"HLA-A*03:01\": \"GSHSMRYFFTSVSRPGRGEPRFIAMGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDRETQKAKGNEQSFRVDLRTLLGYYNQSEDGSHTIQIMYGCDVGPDGRLLRGYDQYAYDGKDYIALNEDLRSWTAADTAAQITQRKWEAARVAEQLRAYLEGTCVEWLRRYLENGKDKLERADPPKTHVTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWE\",\n",
    "    \"HLA-B*08:01\": \"GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQKMEPRAPWIEQEGPEYWDRETQKAKGNEQSFRVDLRTLLGYYNQSEDGSHTIQIMYGCDVGPDGRLLRGYDQYAYDGKDYIALNEDLRSWTAADTAAQITQRKWEAARVAEQLRAYLEGTCVEWLRRYLENGKDKLERADPPKTHVTHHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWE\"\n",
    "}\n",
    "\n",
    "def get_mhc_sequence(mhc_name: str) -> str:\n",
    "    \"\"\"Get MHC amino acid sequence from allele name\"\"\"\n",
    "    return MHC_PSEUDO_SEQUENCES.get(mhc_name, MHC_PSEUDO_SEQUENCES[\"HLA-A*02:01\"])\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    accelerate_set_seed(seed)\n",
    "\n",
    "# --------------------------\n",
    "# Enhanced ESM Encoder\n",
    "# --------------------------\n",
    "\n",
    "class EnhancedESMEncoder:\n",
    "    \"\"\"Enhanced ESM encoder with minimal freezing for maximum accuracy\"\"\"\n",
    "    def __init__(self, model_name: str = \"facebook/esm2_t12_35M_UR50D\", device: str = \"cpu\", \n",
    "                 freeze_layers: int = 2):\n",
    "        print(f\"[Info] Loading Enhanced ESM model: {model_name}\")\n",
    "        self.device = device\n",
    "        self.tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "        self.model = EsmModel.from_pretrained(model_name).to(device)\n",
    "        \n",
    "        # Aggressive fine-tuning - only freeze first 2 layers\n",
    "        self.model.train()\n",
    "        \n",
    "        total_layers = len(self.model.encoder.layer)\n",
    "        trainable_layers = 0\n",
    "        \n",
    "        for i, layer in enumerate(self.model.encoder.layer):\n",
    "            if i < freeze_layers:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            else:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "                trainable_layers += 1\n",
    "        \n",
    "        # Always fine-tune embeddings and pooler for maximum accuracy\n",
    "        for param in self.model.embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.model.pooler.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        self.hidden_size = self.model.config.hidden_size\n",
    "        print(f\"[Info] Enhanced ESM loaded - Hidden size: {self.hidden_size}, Trainable layers: {trainable_layers}/{total_layers}\")\n",
    "    \n",
    "    def encode_batch(self, sequences: List[str], max_length: int = 512) -> torch.Tensor:\n",
    "        \"\"\"Enhanced batch encoding with better preprocessing\"\"\"\n",
    "        if not sequences:\n",
    "            return torch.empty(0, self.hidden_size, device=self.device)\n",
    "        \n",
    "        # Enhanced sequence cleaning\n",
    "        clean_seqs = []\n",
    "        for seq in sequences:\n",
    "            clean_seq = \"\".join([c for c in seq.upper() if c in AA_SET])\n",
    "            clean_seqs.append(clean_seq if clean_seq else \"A\")\n",
    "        \n",
    "        # Batch tokenize with optimized settings\n",
    "        inputs = self.tokenizer(\n",
    "            clean_seqs, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=max_length\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(**inputs)\n",
    "        # Use [CLS] token representation\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        return cls_embeddings\n",
    "\n",
    "# --------------------------\n",
    "# Enhanced Model Architecture\n",
    "# --------------------------\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention for better representation fusion\"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # Transform and reshape\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        context = torch.matmul(attn, v)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        return self.out(context)\n",
    "\n",
    "class EnhancedTCRModel(nn.Module):\n",
    "    \"\"\"Enhanced model with maximum capacity for accuracy\"\"\"\n",
    "    def __init__(self, esm_encoder: EnhancedESMEncoder, d_model: int = 512, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.esm_encoder = esm_encoder\n",
    "        self.esm_hidden_size = esm_encoder.hidden_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Enhanced projection layers with more capacity\n",
    "        self.proj_tcr = nn.Sequential(\n",
    "            nn.Linear(self.esm_hidden_size, d_model * 2),\n",
    "            nn.LayerNorm(d_model * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.proj_mhc = nn.Sequential(\n",
    "            nn.Linear(self.esm_hidden_size, d_model * 2),\n",
    "            nn.LayerNorm(d_model * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.proj_peptide = nn.Sequential(\n",
    "            nn.Linear(self.esm_hidden_size, d_model * 2),\n",
    "            nn.LayerNorm(d_model * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Cross-attention between components\n",
    "        self.tcr_peptide_attention = MultiHeadAttention(d_model, n_heads=8, dropout=dropout)\n",
    "        self.mhc_peptide_attention = MultiHeadAttention(d_model, n_heads=8, dropout=dropout)\n",
    "        \n",
    "        # Enhanced fusion with multiple layers\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(d_model * 5, d_model * 4),  # 5 = tcr + mhc + peptide + 2 attention outputs\n",
    "            nn.LayerNorm(d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model * 2),\n",
    "            nn.LayerNorm(d_model * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Multi-layer classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.LayerNorm(d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, d_model // 4),\n",
    "            nn.LayerNorm(d_model // 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 4, 1)\n",
    "        )\n",
    "        \n",
    "        # InfoNCE projection for contrastive learning\n",
    "        self.infonce_proj = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.LayerNorm(d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 2, d_model // 4)\n",
    "        )\n",
    "        \n",
    "        # Learnable temperature\n",
    "        self.temperature = nn.Parameter(torch.tensor(0.07))\n",
    "        \n",
    "    def forward(self, cdr3_seqs: List[str], mhc_alleles: List[str], peptide_seqs: List[str]) -> Dict[str, torch.Tensor]:\n",
    "        # Convert MHC alleles to sequences\n",
    "        mhc_seqs = [get_mhc_sequence(allele) for allele in mhc_alleles]\n",
    "        \n",
    "        # ESM encoding\n",
    "        cdr3_emb = self.esm_encoder.encode_batch(cdr3_seqs)\n",
    "        mhc_emb = self.esm_encoder.encode_batch(mhc_seqs)\n",
    "        peptide_emb = self.esm_encoder.encode_batch(peptide_seqs)\n",
    "        \n",
    "        # Projection\n",
    "        cdr3_proj = self.proj_tcr(cdr3_emb)\n",
    "        mhc_proj = self.proj_mhc(mhc_emb)\n",
    "        peptide_proj = self.proj_peptide(peptide_emb)\n",
    "        \n",
    "        # Cross-attention\n",
    "        tcr_pep_attn = self.tcr_peptide_attention(\n",
    "            cdr3_proj.unsqueeze(1), peptide_proj.unsqueeze(1), peptide_proj.unsqueeze(1)\n",
    "        ).squeeze(1)\n",
    "        \n",
    "        mhc_pep_attn = self.mhc_peptide_attention(\n",
    "            mhc_proj.unsqueeze(1), peptide_proj.unsqueeze(1), peptide_proj.unsqueeze(1)\n",
    "        ).squeeze(1)\n",
    "        \n",
    "        # Enhanced fusion\n",
    "        combined = torch.cat([cdr3_proj, mhc_proj, peptide_proj, tcr_pep_attn, mhc_pep_attn], dim=-1)\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(fused).squeeze(-1)\n",
    "        \n",
    "        # InfoNCE features\n",
    "        infonce_features = self.infonce_proj(fused)\n",
    "        \n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'infonce_features': infonce_features,\n",
    "            'fused_features': fused\n",
    "        }\n",
    "\n",
    "# --------------------------\n",
    "# Enhanced Loss Functions\n",
    "# --------------------------\n",
    "\n",
    "class InfoNCELoss(nn.Module):\n",
    "    \"\"\"InfoNCE contrastive loss for better representation learning\"\"\"\n",
    "    def __init__(self, temperature: float = 0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, features: torch.Tensor, labels: torch.Tensor, temperature: torch.Tensor):\n",
    "        # Normalize features\n",
    "        features = F.normalize(features, dim=1)\n",
    "        \n",
    "        batch_size = features.shape[0]\n",
    "        if batch_size < 2:\n",
    "            return torch.tensor(0.0, device=features.device)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        sim_matrix = torch.matmul(features, features.t()) / temperature\n",
    "        \n",
    "        # Create positive mask\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.t()).float().to(features.device)\n",
    "        mask = mask - torch.eye(batch_size, device=features.device)  # Remove self-similarity\n",
    "        \n",
    "        # InfoNCE loss\n",
    "        exp_sim = torch.exp(sim_matrix)\n",
    "        sum_exp_sim = exp_sim.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        loss = 0\n",
    "        num_positives = 0\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            pos_sim = exp_sim[i] * mask[i]\n",
    "            if pos_sim.sum() > 0:\n",
    "                loss += -torch.log(pos_sim.sum() / sum_exp_sim[i].clamp(min=1e-8))\n",
    "                num_positives += 1\n",
    "        \n",
    "        return loss / max(1, num_positives)\n",
    "\n",
    "# --------------------------\n",
    "# Enhanced Data Processing (same as before - abbreviated for space)\n",
    "# --------------------------\n",
    "\n",
    "def sliding_window_peptides(seq: str, lengths: List[int]) -> Set[str]:\n",
    "    \"\"\"Extract peptides of specified lengths from sequence\"\"\"\n",
    "    outs = set()\n",
    "    L = len(seq)\n",
    "    for k in lengths:\n",
    "        if k <= 0 or k > L: \n",
    "            continue\n",
    "        for i in range(0, L - k + 1):\n",
    "            p = seq[i:i+k]\n",
    "            if all(ch in AA_SET for ch in p):\n",
    "                outs.add(p)\n",
    "    return outs\n",
    "\n",
    "def enhanced_mutate_peptide(peptide: str, n_mut: int = 1, mutation_strategies: List[str] = None) -> str:\n",
    "    \"\"\"Enhanced peptide mutation with multiple strategies\"\"\"\n",
    "    if mutation_strategies is None:\n",
    "        mutation_strategies = ['random', 'conservative', 'hydrophobic']\n",
    "    \n",
    "    s = list(peptide)\n",
    "    L = len(s)\n",
    "    strategy = random.choice(mutation_strategies)\n",
    "    \n",
    "    # Conservative mutations (similar amino acids)\n",
    "    conservative_groups = {\n",
    "        'A': ['V', 'I', 'L'], 'V': ['A', 'I', 'L'], 'I': ['A', 'V', 'L'], 'L': ['A', 'V', 'I'],\n",
    "        'F': ['Y', 'W'], 'Y': ['F', 'W'], 'W': ['F', 'Y'],\n",
    "        'S': ['T'], 'T': ['S'],\n",
    "        'D': ['E'], 'E': ['D'],\n",
    "        'N': ['Q'], 'Q': ['N'],\n",
    "        'K': ['R'], 'R': ['K']\n",
    "    }\n",
    "    \n",
    "    # Hydrophobic amino acids\n",
    "    hydrophobic = ['A', 'V', 'I', 'L', 'M', 'F', 'Y', 'W']\n",
    "    \n",
    "    n = min(n_mut, L)\n",
    "    positions = random.sample(range(L), n)\n",
    "    \n",
    "    for pos in positions:\n",
    "        original = s[pos]\n",
    "        \n",
    "        if strategy == 'conservative' and original in conservative_groups:\n",
    "            candidates = conservative_groups[original]\n",
    "        elif strategy == 'hydrophobic' and original in hydrophobic:\n",
    "            candidates = hydrophobic\n",
    "        else:  # random\n",
    "            candidates = [aa for aa in AA_STANDARD if aa != original]\n",
    "        \n",
    "        if candidates:\n",
    "            s[pos] = random.choice(candidates)\n",
    "    \n",
    "    return \"\".join(s)\n",
    "\n",
    "def augment_positive_samples(df_positive: pd.DataFrame, target_count: int) -> pd.DataFrame:\n",
    "    \"\"\"Augment positive samples through various data augmentation techniques\"\"\"\n",
    "    augmented_rows = []\n",
    "    current_count = len(df_positive)\n",
    "    \n",
    "    if current_count >= target_count:\n",
    "        return df_positive\n",
    "    \n",
    "    needed_samples = target_count - current_count\n",
    "    print(f\"[Info] Augmenting {needed_samples} positive samples...\")\n",
    "    \n",
    "    # Convert to list for easier sampling\n",
    "    positive_rows = df_positive.to_dict('records')\n",
    "    \n",
    "    for _ in range(needed_samples):\n",
    "        # Randomly select a positive sample to augment\n",
    "        base_row = random.choice(positive_rows)\n",
    "        cdr3, mhc, peptide = base_row[\"CDR3\"], base_row[\"MHC\"], base_row[\"Epitope\"]\n",
    "        \n",
    "        # Choose augmentation strategy\n",
    "        strategy = random.choice(['peptide_mutation', 'cdr3_mutation', 'conservative_mutation'])\n",
    "        \n",
    "        if strategy == 'peptide_mutation':\n",
    "            # Light mutation of peptide (1-2 positions)\n",
    "            new_peptide = enhanced_mutate_peptide(peptide, n_mut=random.choice([1, 2]), \n",
    "                                                 mutation_strategies=['conservative'])\n",
    "            augmented_rows.append({\n",
    "                \"CDR3\": cdr3,\n",
    "                \"MHC\": mhc,\n",
    "                \"Epitope\": new_peptide,\n",
    "                \"label\": 1\n",
    "            })\n",
    "        \n",
    "        elif strategy == 'cdr3_mutation':\n",
    "            # Light mutation of CDR3 (1-2 positions for longer sequences)\n",
    "            if len(cdr3) > 8:\n",
    "                new_cdr3 = enhanced_mutate_peptide(cdr3, n_mut=random.choice([1, 2]), \n",
    "                                                  mutation_strategies=['conservative'])\n",
    "                augmented_rows.append({\n",
    "                    \"CDR3\": new_cdr3,\n",
    "                    \"MHC\": mhc,\n",
    "                    \"Epitope\": peptide,\n",
    "                    \"label\": 1\n",
    "                })\n",
    "            else:\n",
    "                # For shorter CDR3, just duplicate with slight variation in context\n",
    "                augmented_rows.append({\n",
    "                    \"CDR3\": cdr3,\n",
    "                    \"MHC\": mhc,\n",
    "                    \"Epitope\": peptide,\n",
    "                    \"label\": 1\n",
    "                })\n",
    "        \n",
    "        else:  # conservative_mutation\n",
    "            # Very conservative mutation of both peptide and CDR3\n",
    "            new_peptide = enhanced_mutate_peptide(peptide, n_mut=1, \n",
    "                                                 mutation_strategies=['conservative'])\n",
    "            if len(cdr3) > 8:\n",
    "                new_cdr3 = enhanced_mutate_peptide(cdr3, n_mut=1, \n",
    "                                                  mutation_strategies=['conservative'])\n",
    "            else:\n",
    "                new_cdr3 = cdr3\n",
    "            \n",
    "            augmented_rows.append({\n",
    "                \"CDR3\": new_cdr3,\n",
    "                \"MHC\": mhc,\n",
    "                \"Epitope\": new_peptide,\n",
    "                \"label\": 1\n",
    "            })\n",
    "    \n",
    "    # Combine original and augmented samples\n",
    "    df_augmented = pd.DataFrame(augmented_rows)\n",
    "    df_combined = pd.concat([df_positive, df_augmented], axis=0, ignore_index=True)\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    \n",
    "    print(f\"[Info] Augmented positive samples: {current_count} -> {len(df_combined)}\")\n",
    "    return df_combined\n",
    "\n",
    "def enhanced_build_negatives_with_balance(df: pd.DataFrame, k_neg: int = 8, balance_ratio: float = 1.5) -> pd.DataFrame:\n",
    "    \"\"\"Enhanced negative sampling with positive sample balancing\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"label\"] = 1\n",
    "    original_positive_count = len(df)\n",
    "    \n",
    "    # Group peptides by length and properties\n",
    "    peps = df[\"Epitope\"].dropna().astype(str).str.upper().tolist()\n",
    "    peps = [p for p in peps if all(c in AA_SET for c in p)]\n",
    "    \n",
    "    peps_by_len = {}\n",
    "    for p in set(peps):\n",
    "        peps_by_len.setdefault(len(p), []).append(p)\n",
    "    \n",
    "    # Group TCRs by MHC\n",
    "    tcrs_by_mhc = {}\n",
    "    for _, row in df.iterrows():\n",
    "        mhc = str(row[\"MHC\"])\n",
    "        tcr = str(row[\"CDR3\"])\n",
    "        tcrs_by_mhc.setdefault(mhc, []).append(tcr)\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    for _, r in df.iterrows():\n",
    "        c, m, p = str(r[\"CDR3\"]), str(r[\"MHC\"]), str(r[\"Epitope\"])\n",
    "        p = p.upper()\n",
    "        L = len(p)\n",
    "        \n",
    "        # Strategy 1: Different peptides, same length\n",
    "        cand_list = peps_by_len.get(L, [])\n",
    "        neg_peptides = []\n",
    "        \n",
    "        for _ in range(k_neg // 4):\n",
    "            if cand_list:\n",
    "                q = random.choice(cand_list)\n",
    "                if q != p:\n",
    "                    neg_peptides.append(q)\n",
    "        \n",
    "        # Strategy 2: Mutated peptides (multiple mutation levels)\n",
    "        for mut_level in [1, 2, 3]:\n",
    "            for _ in range(k_neg // 4):\n",
    "                neg_peptides.append(enhanced_mutate_peptide(p, n_mut=mut_level))\n",
    "        \n",
    "        # Strategy 3: Cross-MHC negatives\n",
    "        for other_mhc, other_tcrs in tcrs_by_mhc.items():\n",
    "            if other_mhc != m and other_tcrs:\n",
    "                neg_tcr = random.choice(other_tcrs)\n",
    "                rows.append({\"CDR3\": neg_tcr, \"MHC\": m, \"Epitope\": p, \"label\": 0})\n",
    "        \n",
    "        # Add peptide negatives\n",
    "        for q in neg_peptides:\n",
    "            rows.append({\"CDR3\": c, \"MHC\": m, \"Epitope\": q, \"label\": 0})\n",
    "    \n",
    "    df_neg = pd.DataFrame(rows)\n",
    "    negative_count = len(df_neg)\n",
    "    \n",
    "    print(f\"[Info] Generated {negative_count} negative samples for {original_positive_count} positive samples\")\n",
    "    \n",
    "    # Balance positive samples if needed\n",
    "    target_positive_count = max(original_positive_count, int(negative_count / balance_ratio))\n",
    "    \n",
    "    if target_positive_count > original_positive_count:\n",
    "        print(f\"[Info] Balancing dataset: target positive samples = {target_positive_count}\")\n",
    "        df_balanced_positive = augment_positive_samples(df, target_positive_count)\n",
    "    else:\n",
    "        df_balanced_positive = df\n",
    "    \n",
    "    # Combine balanced positive and negative samples\n",
    "    out = pd.concat([df_balanced_positive[[\"CDR3\",\"MHC\",\"Epitope\",\"label\"]], df_neg], axis=0, ignore_index=True)\n",
    "    out = out.drop_duplicates()\n",
    "    \n",
    "    final_positive = len(out[out[\"label\"] == 1])\n",
    "    final_negative = len(out[out[\"label\"] == 0])\n",
    "    \n",
    "    print(f\"[Info] Final balanced dataset: {final_positive} positives + {final_negative} negatives = {len(out)} total\")\n",
    "    print(f\"[Info] Positive/Negative ratio: {final_positive/final_negative:.2f}\")\n",
    "    \n",
    "    return out\n",
    "\n",
    "def split_by_epitope(df_all: pd.DataFrame, seed: int = 42, ratio=(8,1,1)) -> pd.DataFrame:\n",
    "    \"\"\"Split data by epitope to avoid data leakage\"\"\"\n",
    "    train_r, val_r, test_r = ratio\n",
    "    peps = df_all[df_all[\"label\"]==1][\"Epitope\"].drop_duplicates().sample(frac=1.0, random_state=seed).tolist()\n",
    "    n = len(peps)\n",
    "    n_train = int(n * train_r / sum(ratio))\n",
    "    n_val = int(n * val_r / sum(ratio))\n",
    "    train_peps = set(peps[:n_train])\n",
    "    val_peps = set(peps[n_train:n_train+n_val])\n",
    "    \n",
    "    def split_label(p):\n",
    "        if p in train_peps: return \"train\"\n",
    "        if p in val_peps: return \"val\"\n",
    "        return \"test\"\n",
    "    \n",
    "    df_all = df_all.copy()\n",
    "    df_all[\"split\"] = df_all[\"Epitope\"].map(split_label)\n",
    "    return df_all\n",
    "\n",
    "# --------------------------\n",
    "# Enhanced Dataset\n",
    "# --------------------------\n",
    "\n",
    "class EnhancedDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        r = self.df.iloc[idx]\n",
    "        return {\n",
    "            \"cdr3\": str(r[\"CDR3\"]),\n",
    "            \"mhc\": str(r[\"MHC\"]), \n",
    "            \"peptide\": str(r[\"Epitope\"]),\n",
    "            \"label\": float(r[\"label\"]) if \"label\" in self.df.columns else 1.0\n",
    "        }\n",
    "    \n",
    "    def collate_fn(self, batch: List[Dict]) -> Dict:\n",
    "        cdr3_seqs = [item[\"cdr3\"] for item in batch]\n",
    "        mhc_alleles = [item[\"mhc\"] for item in batch]\n",
    "        peptide_seqs = [item[\"peptide\"] for item in batch]\n",
    "        labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "            \"cdr3_seqs\": cdr3_seqs,\n",
    "            \"mhc_alleles\": mhc_alleles,\n",
    "            \"peptide_seqs\": peptide_seqs,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# --------------------------\n",
    "# Fixed Training with Logging\n",
    "# --------------------------\n",
    "\n",
    "def evaluate_model(model, dataloader, bce_loss, infonce_loss, device):\n",
    "    \"\"\"Evaluate model and return detailed metrics\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_bce = 0.0\n",
    "    total_infonce = 0.0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(batch[\"cdr3_seqs\"], batch[\"mhc_alleles\"], batch[\"peptide_seqs\"])\n",
    "            \n",
    "            bce = bce_loss(outputs['logits'], labels)\n",
    "            # Fixed: Use helper function to get temperature\n",
    "            temperature = get_model_attr(model, 'temperature')\n",
    "            infonce = infonce_loss(outputs['infonce_features'], labels, temperature)\n",
    "            \n",
    "            total_loss += bce.item() + 0.3 * infonce.item()  # Weighted combination\n",
    "            total_bce += bce.item()\n",
    "            total_infonce += infonce.item()\n",
    "            \n",
    "            all_logits.extend(outputs['logits'].cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute metrics\n",
    "    logits_array = np.array(all_logits)\n",
    "    labels_array = np.array(all_labels)\n",
    "    probs_array = torch.sigmoid(torch.tensor(logits_array)).numpy()\n",
    "    \n",
    "    cv_evaluator = CrossValidationEvaluator()\n",
    "    metrics = cv_evaluator.compute_detailed_metrics(labels_array, probs_array)\n",
    "    \n",
    "    metrics.update({\n",
    "        'total_loss': total_loss / len(dataloader),\n",
    "        'bce_loss': total_bce / len(dataloader),\n",
    "        'infonce_loss': total_infonce / len(dataloader)\n",
    "    })\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_enhanced_model_with_logging(df_train: pd.DataFrame, df_val: pd.DataFrame, esm_encoder: EnhancedESMEncoder, \n",
    "                                    d_model=512, lr=1e-4, batch_size=16, epochs=25, log_dir=\"training_logs\"):\n",
    "    \"\"\"Enhanced training with comprehensive logging\"\"\"\n",
    "    \n",
    "    # Initialize accelerator with BF16\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision='bf16',\n",
    "        gradient_accumulation_steps=2,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=log_dir\n",
    "    )\n",
    "    \n",
    "    device = accelerator.device\n",
    "    print(f\"[Info] Training Enhanced Model with Logging - Device: {device}\")\n",
    "    print(f\"[Info] Mixed Precision: {accelerator.mixed_precision}\")\n",
    "    print(f\"[Info] Epochs: {epochs}, Batch Size: {batch_size}, Model Dim: {d_model}\")\n",
    "    \n",
    "    # Initialize logger\n",
    "    logger = TrainingLogger(log_dir=log_dir)\n",
    "    \n",
    "    # Move ESM encoder to correct device\n",
    "    esm_encoder.model = esm_encoder.model.to(device)\n",
    "    esm_encoder.device = device\n",
    "    \n",
    "    model = EnhancedTCRModel(esm_encoder, d_model=d_model).to(device)\n",
    "    \n",
    "    # Enhanced loss functions\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "    infonce_loss = InfoNCELoss()\n",
    "    \n",
    "    # Advanced optimizer with different learning rates\n",
    "    esm_params = []\n",
    "    proj_params = []\n",
    "    attn_params = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'esm_encoder' in name:\n",
    "            esm_params.append(param)\n",
    "        elif 'attention' in name:\n",
    "            attn_params.append(param)\n",
    "        else:\n",
    "            proj_params.append(param)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': esm_params, 'lr': lr * 0.05, 'weight_decay': 0.01},\n",
    "        {'params': attn_params, 'lr': lr * 0.5, 'weight_decay': 0.05},\n",
    "        {'params': proj_params, 'lr': lr, 'weight_decay': 0.1}\n",
    "    ])\n",
    "    \n",
    "    # Enhanced scheduler with warmup\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=epochs//3, T_mult=2, eta_min=1e-7\n",
    "    )\n",
    "    \n",
    "    train_dataset = EnhancedDataset(df_train)\n",
    "    val_dataset = EnhancedDataset(df_val)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=train_dataset.collate_fn, \n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        collate_fn=val_dataset.collate_fn, \n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Prepare everything with accelerator\n",
    "    model, optimizer, train_loader, val_loader, scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader, scheduler\n",
    "    )\n",
    "    \n",
    "    best_val_auc = 0.0\n",
    "    best_model_state = None\n",
    "    patience = 8\n",
    "    no_improve = 0\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_bce = 0.0\n",
    "        train_infonce = 0.0\n",
    "        train_steps = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch} Training\", disable=not accelerator.is_local_main_process)\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            with accelerator.accumulate(model):\n",
    "                labels = batch[\"labels\"]\n",
    "                \n",
    "                outputs = model(batch[\"cdr3_seqs\"], batch[\"mhc_alleles\"], batch[\"peptide_seqs\"])\n",
    "                \n",
    "                # Combined loss with InfoNCE\n",
    "                bce = bce_loss(outputs['logits'], labels)\n",
    "                # Fixed: Use helper function to get temperature\n",
    "                temperature = get_model_attr(model, 'temperature')\n",
    "                infonce = infonce_loss(outputs['infonce_features'], labels, temperature)\n",
    "                \n",
    "                # Dynamic weight for InfoNCE\n",
    "                infonce_weight = min(0.5, 0.1 + 0.4 * epoch / epochs)\n",
    "                total_loss = bce + infonce_weight * infonce\n",
    "                \n",
    "                # Backward pass with accelerator\n",
    "                accelerator.backward(total_loss)\n",
    "                \n",
    "                # Gradient clipping\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Accumulate losses\n",
    "                train_loss += total_loss.item()\n",
    "                train_bce += bce.item()\n",
    "                train_infonce += infonce.item()\n",
    "                train_steps += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                if accelerator.is_local_main_process:\n",
    "                    progress_bar.set_postfix({\n",
    "                        'loss': f\"{total_loss.item():.4f}\",\n",
    "                        'bce': f\"{bce.item():.4f}\",\n",
    "                        'infonce': f\"{infonce.item():.4f}\"\n",
    "                    })\n",
    "        \n",
    "        train_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Validation\n",
    "        val_start_time = time.time()\n",
    "        val_metrics = evaluate_model(model, val_loader, bce_loss, infonce_loss, device)\n",
    "        val_time = time.time() - val_start_time\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_train_loss = train_loss / max(1, train_steps)\n",
    "        avg_train_bce = train_bce / max(1, train_steps)\n",
    "        avg_train_infonce = train_infonce / max(1, train_steps)\n",
    "        \n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        current_temp = get_model_attr(model, 'temperature').item()\n",
    "        \n",
    "        # Log epoch data\n",
    "        if accelerator.is_local_main_process:\n",
    "            epoch_data = {\n",
    "                'epoch': epoch,\n",
    "                'train_loss': avg_train_loss,\n",
    "                'train_bce_loss': avg_train_bce,\n",
    "                'train_infonce_loss': avg_train_infonce,\n",
    "                'val_loss': val_metrics['total_loss'],\n",
    "                'val_bce_loss': val_metrics['bce_loss'],\n",
    "                'val_infonce_loss': val_metrics['infonce_loss'],\n",
    "                'val_auc': val_metrics['auc'],\n",
    "                'val_auprc': val_metrics['auprc'],\n",
    "                'val_accuracy': val_metrics['accuracy'],\n",
    "                'val_precision': val_metrics['precision'],\n",
    "                'val_recall': val_metrics['recall'],\n",
    "                'val_f1': val_metrics['f1'],\n",
    "                'learning_rate': current_lr,\n",
    "                'temperature': current_temp,\n",
    "                'train_time': train_time,\n",
    "                'val_time': val_time\n",
    "            }\n",
    "            \n",
    "            logger.log_epoch(epoch_data)\n",
    "            \n",
    "            print(f\"Epoch {epoch}:\")\n",
    "            print(f\"  Train: Loss={avg_train_loss:.4f}, BCE={avg_train_bce:.4f}, InfoNCE={avg_train_infonce:.4f}\")\n",
    "            print(f\"  Val: Loss={val_metrics['total_loss']:.4f}, AUC={val_metrics['auc']:.4f}, AUPRC={val_metrics['auprc']:.4f}\")\n",
    "            print(f\"  Val: Acc={val_metrics['accuracy']:.4f}, F1={val_metrics['f1']:.4f}, Prec={val_metrics['precision']:.4f}, Rec={val_metrics['recall']:.4f}\")\n",
    "            print(f\"  LR={current_lr:.2e}, Temp={current_temp:.4f}, Time={train_time:.1f}s+{val_time:.1f}s\")\n",
    "            \n",
    "            # Early stopping based on AUC\n",
    "            if val_metrics['auc'] > best_val_auc:\n",
    "                best_val_auc = val_metrics['auc']\n",
    "                best_model_state = accelerator.get_state_dict(model)\n",
    "                no_improve = 0\n",
    "                print(f\"  ✓ New best model! AUC: {best_val_auc:.4f}\")\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(f\"  Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "        \n",
    "        # Wait for all processes\n",
    "        accelerator.wait_for_everyone()\n",
    "    \n",
    "    # Save training logs and plots\n",
    "    if accelerator.is_local_main_process:\n",
    "        logger.save_logs()\n",
    "        logger.save_json()\n",
    "        logger.plot_training_curves()\n",
    "    \n",
    "    # Load best model\n",
    "    if accelerator.is_local_main_process and best_model_state:\n",
    "        accelerator.load_state(model, best_model_state)\n",
    "        print(f\"[Info] Loaded best model with AUC: {best_val_auc:.4f}\")\n",
    "    \n",
    "    accelerator.wait_for_everyone()\n",
    "    \n",
    "    # Return unwrapped model and best AUC\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    return unwrapped_model, best_val_auc\n",
    "\n",
    "# --------------------------\n",
    "# 5-Fold Cross-Validation Implementation\n",
    "# --------------------------\n",
    "\n",
    "def run_cross_validation(df_all: pd.DataFrame, esm_model_name: str = \"facebook/esm2_t12_35M_UR50D\",\n",
    "                        d_model: int = 512, lr: float = 1e-4, batch_size: int = 16, epochs: int = 15):\n",
    "    \"\"\"Run 5-fold cross-validation with detailed logging\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RUNNING 5-FOLD CROSS-VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize cross-validation evaluator\n",
    "    cv_evaluator = CrossValidationEvaluator(n_splits=5, random_state=42)\n",
    "    \n",
    "    # Get unique epitopes for splitting\n",
    "    positive_df = df_all[df_all[\"label\"] == 1].copy()\n",
    "    unique_epitopes = positive_df[\"Epitope\"].unique()\n",
    "    \n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_epitope_idx, val_epitope_idx) in enumerate(kfold.split(unique_epitopes)):\n",
    "        print(f\"\\n[Fold {fold + 1}/5] Starting training...\")\n",
    "        \n",
    "        # Split epitopes\n",
    "        train_epitopes = set(unique_epitopes[train_epitope_idx])\n",
    "        val_epitopes = set(unique_epitopes[val_epitope_idx])\n",
    "        \n",
    "        # Split data based on epitopes\n",
    "        train_mask = df_all[\"Epitope\"].isin(train_epitopes)\n",
    "        val_mask = df_all[\"Epitope\"].isin(val_epitopes)\n",
    "        \n",
    "        df_train_fold = df_all[train_mask].reset_index(drop=True)\n",
    "        df_val_fold = df_all[val_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"[Fold {fold + 1}] Train: {len(df_train_fold)} samples, Val: {len(df_val_fold)} samples\")\n",
    "        \n",
    "        # Initialize ESM encoder for this fold\n",
    "        esm_encoder = EnhancedESMEncoder(\n",
    "            model_name=esm_model_name,\n",
    "            device=\"cpu\",  # Will be handled by accelerator\n",
    "            freeze_layers=2\n",
    "        )\n",
    "        \n",
    "        # Train model for this fold\n",
    "        fold_log_dir = f\"training_logs/fold_{fold + 1}\"\n",
    "        model, best_auc = train_enhanced_model_with_logging(\n",
    "            df_train_fold, df_val_fold, esm_encoder,\n",
    "            d_model=d_model, lr=lr, batch_size=batch_size, epochs=epochs,\n",
    "            log_dir=fold_log_dir\n",
    "        )\n",
    "        \n",
    "        # Move model to CPU for evaluation\n",
    "        model = model.cpu()\n",
    "        model.esm_encoder.device = \"cpu\"\n",
    "        model.esm_encoder.model = model.esm_encoder.model.cpu()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_dataset = EnhancedDataset(df_val_fold)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=val_dataset.collate_fn)\n",
    "        \n",
    "        model.eval()\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                labels = batch[\"labels\"]\n",
    "                outputs = model(batch[\"cdr3_seqs\"], batch[\"mhc_alleles\"], batch[\"peptide_seqs\"])\n",
    "                \n",
    "                all_logits.extend(outputs['logits'].cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        # Compute metrics for this fold\n",
    "        logits_array = np.array(all_logits)\n",
    "        labels_array = np.array(all_labels)\n",
    "        probs_array = torch.sigmoid(torch.tensor(logits_array)).numpy()\n",
    "        \n",
    "        fold_metrics = cv_evaluator.compute_detailed_metrics(labels_array, probs_array)\n",
    "        fold_metrics['best_training_auc'] = best_auc\n",
    "        \n",
    "        cv_evaluator.add_fold_result(fold + 1, fold_metrics)\n",
    "        \n",
    "        print(f\"[Fold {fold + 1}] Results:\")\n",
    "        print(f\"  AUC: {fold_metrics['auc']:.4f}\")\n",
    "        print(f\"  AUPRC: {fold_metrics['auprc']:.4f}\")\n",
    "        print(f\"  Accuracy: {fold_metrics['accuracy']:.4f}\")\n",
    "        print(f\"  F1: {fold_metrics['f1']:.4f}\")\n",
    "        print(f\"  Precision: {fold_metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {fold_metrics['recall']:.4f}\")\n",
    "        \n",
    "        # Clean up memory\n",
    "        del model, esm_encoder\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save and display results\n",
    "    cv_results_path = cv_evaluator.save_results(\"cv_results.json\")\n",
    "    cv_plot_path = cv_evaluator.plot_cv_results(\"cv_results.png\")\n",
    "    \n",
    "    # Display final summary\n",
    "    summary = cv_evaluator.get_summary_with_ci()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"5-FOLD CROSS-VALIDATION RESULTS WITH 95% CONFIDENCE INTERVALS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for metric, stats in summary.items():\n",
    "        mean = stats['mean']\n",
    "        ci_lower = stats['ci_lower']\n",
    "        ci_upper = stats['ci_upper']\n",
    "        std = stats['std']\n",
    "        \n",
    "        print(f\"{metric.upper():>10}: {mean:.4f} ± {std:.4f} (CI: [{ci_lower:.4f}, {ci_upper:.4f}])\")\n",
    "    \n",
    "    return cv_evaluator, summary\n",
    "\n",
    "# --------------------------\n",
    "# Main Enhanced Pipeline\n",
    "# --------------------------\n",
    "\n",
    "def main():\n",
    "    set_seed(42)\n",
    "    \n",
    "    print(\"[Info] Enhanced TCR Analysis Pipeline with Logging and Cross-Validation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load data\n",
    "    if not os.path.exists(\"data.csv\"):\n",
    "        raise FileNotFoundError(\"data.csv not found! Please ensure your data file exists.\")\n",
    "    \n",
    "    print(\"[Info] Loading training data...\")\n",
    "    df = pd.read_csv(\"data.csv\")\n",
    "    required_cols = {\"CDR3\", \"MHC\", \"Epitope\"}\n",
    "    if not required_cols.issubset(set(df.columns)):\n",
    "        raise ValueError(f\"data.csv must contain columns: {required_cols}\")\n",
    "    \n",
    "    # Enhanced data cleaning\n",
    "    df[\"CDR3\"] = df[\"CDR3\"].astype(str).str.upper()\n",
    "    df[\"MHC\"] = df[\"MHC\"].astype(str).str.upper()\n",
    "    df[\"Epitope\"] = df[\"Epitope\"].astype(str).str.upper()\n",
    "    df = df.dropna().drop_duplicates()\n",
    "    df = df[df[\"Epitope\"].map(lambda p: all(ch in AA_SET for ch in p))]\n",
    "    df = df[df[\"CDR3\"].map(lambda p: all(ch in AA_SET for ch in p))]\n",
    "    \n",
    "    print(f\"[Info] Loaded {len(df)} training examples\")\n",
    "    \n",
    "    # Enhanced negative sampling with balanced positive augmentation\n",
    "    print(\"[Step] Generating enhanced negative samples with positive balancing...\")\n",
    "    df_all = enhanced_build_negatives_with_balance(df, k_neg=10, balance_ratio=1.2)\n",
    "    \n",
    "    # Run 5-fold cross-validation\n",
    "    print(\"[Step] Running 5-fold cross-validation...\")\n",
    "    cv_evaluator, cv_summary = run_cross_validation(\n",
    "        df_all, \n",
    "        esm_model_name=\"facebook/esm2_t12_35M_UR50D\",\n",
    "        d_model=128,\n",
    "        lr=1e-4,\n",
    "        batch_size=64,\n",
    "        epochs=10 \n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✅ ENHANCED ANALYSIS WITH LOGGING AND CV COMPLETED!\")\n",
    "    print(\"📊 Cross-Validation Results:\")\n",
    "    for metric, stats in cv_summary.items():\n",
    "        print(f\"   {metric.upper()}: {stats['mean']:.4f} ± {stats['std']:.4f} (CI: [{stats['ci_lower']:.4f}, {stats['ci_upper']:.4f}])\")\n",
    "    print(\"📁 Generated Files:\")\n",
    "    print(\"   • training_logs/ - Training history and plots\")\n",
    "    print(\"   • cv_results.json - Cross-validation results\")\n",
    "    print(\"   • cv_results.png - Cross-validation plot\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e071e3d-8600-49e0-b92c-72ba2f6c1613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
